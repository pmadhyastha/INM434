{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPySiL8dUcKg5HXAcbeIWEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/tokenisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic text processing (pre-processing)"
      ],
      "metadata": {
        "id": "o9csEoM4uQ7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = \"Pranava Madhyastha\" \n",
        "__version__ = \"INM434/IN3045 City, University of London, Spring 2023\""
      ],
      "metadata": {
        "id": "IbzjCqsrupf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents\n",
        "\n",
        "\n",
        "1. [Tokenisation: a brief overview](#Tokenisation)\n",
        "2. [Simple tokeniser](#Simple)\n",
        "3. [Morphological Analyser](#MorphoAnalyser)\n",
        "    1. [Finite state transducer (FST)](#FST)\n",
        "4. [Stemming](#stemming)\n",
        "    1. [Porter stemmer](#porter-stemmer)\n",
        "5. [Subword tokeniser](#subwords)\n",
        "    1. [Byte pair encoding](#bpe)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "HfyIpShRu-kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation: a brief overview\n",
        "\n",
        "This is our first notebook for the module and we are going to focus on tokenisation. Please make sure to login to your respective google account to get started. \n",
        "\n",
        "A tokenizer is used to split an input string into separate tokens or pieces, where each token represents a meaningful element in the input string. This is one of the most important steps in NLP. This step is also called text-preprocessing. \n",
        "\n",
        "Tokenisation helps create a vocabulary of unique tokens overwhich we will run a variety of NLP algorithms. It mainly helps standardise the input data and help us squeeze in information from the long tail. It, in some cases, helps us to break the input text down into smaller, sometimes linguistically meaningful, units. \n",
        "\n",
        "In this notebook, we will first build a simple tokeniser which only splits on white space. We will then look at a toy morphological analyser. We will then build a simplified version of subword based tokeniser. \n",
        "\n",
        "---\n",
        "\n",
        "How does this notebook work: \n",
        "\n",
        "  * There will be some cells with \"TODO\": you will have to fill the code for the placeholder \"YOUR CODE HERE\".\n",
        "\n",
        "  * Each cell should take a few seconds to run, so if it is taking longer, there may be bug. "
      ],
      "metadata": {
        "id": "Dyly6DU1xPLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tokeniser \n",
        "\n",
        "We will make use of the built in regular expression library in python to construct this tokeniser. Please refer to Chapter 2 (SLP: Jurafsky and Martin) references on regular expressions. \n",
        "\n",
        "The tokeniser below will take split input text into separate words based on word boundaries. This is defined using the regular expression pattern \"\\b\\w+\\b\". The re.findall() function is used to extract all the tokens from the text that match the pattern, and the list of tokens is returned. \n"
      ],
      "metadata": {
        "id": "7DLiIkEo2m7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenise(text):\n",
        "    # Define a regular expression pattern to split the text into tokens\n",
        "    pattern = r'\\b\\w+\\b'\n",
        "    \n",
        "    # Use the re.findall() function to extract all the tokens from the text\n",
        "    tokens = re.findall(pattern, text)\n",
        "    \n",
        "    # Return the list of tokens\n",
        "    return tokens\n",
        "\n",
        "# Sample input for the tokeniser\n",
        "text = \"This is IN 3045/INM 434 Natural Language Procssing. This is some sample text for tokeniser\"\n",
        "tokens = tokenise(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "J6QTzwBT2UOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: What does '\\b\\w+\\b' mean? \n",
        "\n",
        "\"\\b\" is a word boundary, \"\\w\" is a shorthand character class that matches a \"word character\" (alphanumeric character or underscore), and \"+\" is a quantifier that matches one or more of the preceding token.\n"
      ],
      "metadata": {
        "id": "jqySFAEX4cXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Morphological analyser\n",
        "\n",
        "We are now going to build a morphological analyser. A morphological analyser allows us to determine the morphological structure of a word, it outputs the inner structure of words with respect to the word's root and affixes. \n",
        "\n",
        "We will build the morphological analyser for \"rats\" => where the analyser will output the root and the operation with the affix. "
      ],
      "metadata": {
        "id": "hpdB6Im_4z55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tCmyXZvu1yk"
      },
      "outputs": [],
      "source": [
        "# Define the FST transitions\n",
        "transitions = {\n",
        "    (0, 'r', 1, 'r', 'R'),\n",
        "    (1, 'a', 2, 'a', 'A'),\n",
        "    (2, 't', 3, 't', 'T'),\n",
        "    (3, 's', 3, 's', '+Plural(S)'),\n",
        "}\n",
        "\n",
        "# Define the FST accepting states\n",
        "accepting_states = [2,3]\n",
        "\n",
        "# Define the initial state\n",
        "initial_state = 0\n",
        "\n",
        "def morphological_analyzer(word):\n",
        "    # Initialize the current state and output\n",
        "    current_state = initial_state\n",
        "    output = \"\"\n",
        "    \n",
        "    # Iterate over each character in the word\n",
        "    for char in word:\n",
        "        # Check if a transition exists for the current state and character\n",
        "        for start_state, in_char, end_state, out_char, out_string in transitions:\n",
        "            if current_state == start_state and char == in_char:\n",
        "                current_state = end_state\n",
        "                output += out_string\n",
        "                break\n",
        "    \n",
        "    # Check if the current state is an accepting state\n",
        "    if current_state in accepting_states:\n",
        "        return output\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Morphological analyser for rats \n",
        "word = \"rats\"\n",
        "output = morphological_analyzer(word)\n",
        "if output is not None:\n",
        "    print(\"The morphological analysis of '{}' is '{}'\".format(word, output))\n",
        "else:\n",
        "    print(\"No morphological analysis found for '{}'\".format(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Extend the code to allow for other examples covered in the class. \n",
        "* leaves -> regular verb leave + plural operation. \n",
        "* leaf -> irregular noun + plural operation. \n",
        "\n",
        "What are the problems with this approach? "
      ],
      "metadata": {
        "id": "VnkPzSqWBfqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming \n",
        "Stemming is a simpler type of word splitting. Their core structure is retained regardless of the actual meaning of the word. This is usually done by removing any affixes (suffix/prefix/inflections), from words. Stemming is a simple, approximate and faster way to reduce words to their root form, but it can lead to over-generalization, where words with different meanings are reduced to the same form."
      ],
      "metadata": {
        "id": "KsTdleL7B-82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_stemmer(word):\n",
        "    # normalise the word by lowercasing\n",
        "    word = word.lower()\n",
        "\n",
        "    # we will only work over words > 2 characters.\n",
        "    if len(word) <= 2:\n",
        "        return word\n",
        "\n",
        "    suffixes = ['al', 'ance', 'ence', 'able', 'ible', 'ment', 'ant', 'ent', 'ism',\n",
        "                'ate', 'iti', 'ous', 'ive', 'ize']\n",
        "\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)] \n",
        "            break # TODO: why are we breaking here? (there are better ways of doing this)\n",
        "\n",
        "    return word\n",
        "\n",
        "# test this function for \"capital\" \n",
        "print(simple_stemmer(\"capital\"))"
      ],
      "metadata": {
        "id": "j8hbNTC5DINK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Extend the list of suffixes, can you come up with other cases? \n",
        "\n",
        "Read porter stemmer algorithm and see what additional changes are necessary to make `simple_stemmer` better at stemming? "
      ],
      "metadata": {
        "id": "36HEdlqqE0QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte pair encoding \n",
        "\n",
        "Sennrich, Rico, Barry Haddow, and Alexandra Birch. \"Neural Machine Translation of Rare Words with Subword Units.\" Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016. http://www.aclweb.org/anthology/P16-1162\n",
        "\n",
        "Main source: https://github.com/rsennrich/subword-nmt\n",
        "\n",
        "Below is a simple version of the algorithm. \n",
        "\n",
        "Two stages: token learner and token segmenter. \n",
        "\n",
        "Let us first look at token learner, this involves: \n",
        "\n",
        "*  computing the frequencies of all words in a corpus (we do it synthetically here) \n",
        "*  start with characters as the basic vocab (characters seen in the corpus)\n",
        "* to obtain vocabulary of n-merge operations: \n",
        "    - Obtain most frequent pairs of characters in the corpus\n",
        "    - add the pair to the list of merges\n",
        "    - add merged characters to the vocab \n",
        "* iterate n times\n",
        "\n",
        "The code below performs this operation. "
      ],
      "metadata": {
        "id": "HH1vbuPHVizw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import collections\n",
        "\n",
        "def compute_symbol_pairs_frequency(vocab):\n",
        "    #Compute the frequency of adjacent symbol pairs in the vocabulary.\n",
        "    \n",
        "    symbol_pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            symbol_pairs[symbols[i], symbols[i+1]] += freq\n",
        "    return symbol_pairs\n",
        "\n",
        "def merge_vocabulary(symbol_pair, vocab_in):\n",
        "    #Merge the given symbol pair in the vocabulary.\n",
        "\n",
        "    vocab_out = {}\n",
        "    bigram = re.escape(' '.join(symbol_pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in vocab_in:\n",
        "        word_out = pattern.sub(''.join(symbol_pair), word)\n",
        "        vocab_out[word_out] = vocab_in[word]\n",
        "    return vocab_out\n",
        "\n",
        "def extract_symbol_pairs(word):\n",
        "    #Return a set of symbol pairs in a word.\n",
        "\n",
        "    symbol_pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        symbol_pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return symbol_pairs\n",
        "\n",
        "train_data = {'n a t u r a l </w>': 5, 'n a t u r e </w>': 2, 'l a n g u a g e </w>': 6, 'l a n g u a g e s </w>': 3}\n",
        "\n",
        "bpe_codes = {}\n",
        "bpe_codes_reverse = {}\n",
        "\n",
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = compute_symbol_pairs_frequency(train_data)\n",
        "    best_pair = max(pairs, key=pairs.get)\n",
        "    train_data = merge_vocabulary(best_pair, train_data)\n",
        "    \n",
        "    bpe_codes[best_pair] = i\n",
        "    bpe_codes_reverse[best_pair[0] + best_pair[1]] = best_pair\n",
        "    \n",
        "    print(\"new merge: {}\".format(best_pair))\n",
        "    print(\"train data: {}\".format(train_data))\n"
      ],
      "metadata": {
        "id": "9U_G2H7j7lpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying BPE to encode a new word\n",
        "\n",
        "- get all character bigrams in the word. \n",
        "- find the pair of char. which appears among the char merges\n",
        "- apply the merge on the word. "
      ],
      "metadata": {
        "id": "vhjsPpdEW626"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpe_encode(input_word):\n",
        "    # Encode the input word using BPE (Byte Pair Encoding) algorithm.\n",
        "\n",
        "    word_list = list(input_word) + ['</w>']\n",
        "    pairs = get_bigrams(word_list)\n",
        "\n",
        "    if not pairs:\n",
        "        return input_word\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        bigram = min(pairs, key=lambda pair: bpe_codes.get(pair, float('inf')))\n",
        "        if bigram not in bpe_codes:\n",
        "            break\n",
        "        first, second = bigram\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word_list):\n",
        "            try:\n",
        "                j = word_list.index(first, i)\n",
        "                new_word.extend(word_list[i:j])\n",
        "                i = j\n",
        "            except:\n",
        "                new_word.extend(word_list[i:])\n",
        "                break\n",
        "\n",
        "            if word_list[i] == first and i < len(word_list) - 1 and word_list[i + 1] == second:\n",
        "                new_word.append(first + second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word_list[i])\n",
        "                i += 1\n",
        "        new_word = tuple(new_word)\n",
        "        word_list = new_word\n",
        "        if len(word_list) == 1:\n",
        "            break\n",
        "        else:\n",
        "            pairs = get_bigrams(word_list)\n",
        "\n",
        "    if word_list[-1] == '</w>':\n",
        "        word_list = word_list[:-1]\n",
        "    elif word_list[-1].endswith('</w>'):\n",
        "        word_list = word_list[:-1] + (word_list[-1].replace('</w>', ''),)\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def get_bigrams(word_list):\n",
        "    # Get all bigrams in the input word list.\n",
        "    return [(word_list[i], word_list[i + 1]) for i in range(len(word_list) - 1)]\n",
        "\n",
        "\n",
        "# Try this: \n",
        "\n",
        "print(bpe_encode(\"naturalised\"))\n"
      ],
      "metadata": {
        "id": "SQcLCuQ9cKzQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}