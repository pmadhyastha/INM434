{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbouD0216hYq7g2xHXeHrD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/ngram_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Math, HTML\n",
        "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "               \"latest.js?config=default'></script>\"))"
      ],
      "metadata": {
        "id": "Q1kG1Wx9LJi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language models\n",
        "\n",
        "As we noted in the lecture today, language models are models that are trained to predict the likelihood of a sequence of words or characters in a language. The primary goal of language models is to learn the structure and patterns of a language by analyzing large amounts of text. \n",
        "\n",
        "\n",
        "We will begin our exploration by looking at the key fundamentals of the simplest language model. \n",
        "\n",
        "We will start of with a toy passage from Simple English Wikipedia. "
      ],
      "metadata": {
        "id": "2yNW9m8zEztF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our toy text (ideally we would train over large amounto of data, this is just a tiny snapshot)\n",
        "input_text = \"\"\"This is the front page of the Simple English Wikipedia. \n",
        "                Wikipedias are places where people work together to write encyclopedias \n",
        "                in different languages. We use Simple English words and grammar here.\n",
        "                The Simple English Wikipedia is for everyone! That includes children and \n",
        "                adults who are learning English. There are 227,530 articles on the Simple \n",
        "                English Wikipedia. All of the pages are free to use.\"\"\""
      ],
      "metadata": {
        "id": "HY-kSILPE7Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram language model \n",
        "\n",
        "Recall that the unigram language model is the simplest version of the language model, where we use the chain rule of probabilities and make a (very bad) independence assumption: we consider only the frequency (normalise this and obtain the probability) of individual words in a text and assume that each word is independent of other words (or contexts) in the sentence. \n",
        "\n",
        "To build a unigram model, the text is first split into individual words, and the frequency of each word is calculated. Then, the probability of each word is calculated by dividing its frequency by the total number of words in the text. This gives a probability distribution over the entire vocabulary of the language.\n",
        "\n",
        "Given a new text, a unigram model can predict the likelihood of each word in the vocabulary based on its frequency. To generate a sentence using a unigram model, words are sampled randomly from the unigram distribution. Given sufficient amount of data this can result in sentences that are grammatically correct but may not make sense semantically.\n",
        "\n",
        "In the code below: a sentence of length 10 is sampled from the unigram distribution using the choices() method from the random module. "
      ],
      "metadata": {
        "id": "5UbxMGBNFvt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Split the text into a list of words\n",
        "words = input_text.split()\n",
        "\n",
        "# Count the frequency of each word and calculate the unigram probabilities\n",
        "unigram_counts = {}\n",
        "for word in words:\n",
        "    if word in unigram_counts:\n",
        "        unigram_counts[word] += 1\n",
        "    else:\n",
        "        unigram_counts[word] = 1\n",
        "\n",
        "num_words = len(words)\n",
        "unigram_probs = {word: count / num_words for word, count in unigram_counts.items()}\n",
        "\n",
        "# Sample a sentence of length 10 from the unigram distribution\n",
        "sampled_sentence = []\n",
        "for i in range(10):\n",
        "    sampled_word = random.choices(list(unigram_probs.keys()), list(unigram_probs.values()))[0]\n",
        "    sampled_sentence.append(sampled_word)\n",
        "\n",
        "# Print out the sampled sentence\n",
        "print(\"Sampled sentence:\")\n",
        "print(\" \".join(sampled_sentence))"
      ],
      "metadata": {
        "id": "Q9KQQrbIFp8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: \n",
        "\n",
        "1.   Try using a large sized input sample and see how the sampling changes. \n",
        "2.   Change the length of the generated samples \n",
        "3.   What happens to words that are not seen in the training set? \n",
        "4.   There is something interesting happening in the `else` loop above. What is this? Why is this done? What is the effect of not having this loop? \n",
        "\n"
      ],
      "metadata": {
        "id": "fGdzmOzWG0Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram language model\n",
        "\n",
        "Recall that the bigram language model predicts the probability of a word based on the probability of its preceding word. In other words, a bigram model considers the probability of a word based on the occurrence of its preceding word in the sentence.\n",
        "\n",
        "Notice in the code below the bigram counts are calculated by looping over each adjacent pair of words in the text. The frequency of each bigram is stored in a dictionary called bigram_counts."
      ],
      "metadata": {
        "id": "kQwlz0_fHb6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate bigram counts\n",
        "bigram_counts = {}\n",
        "for i in range(len(words)-1):\n",
        "    current_word = words[i]\n",
        "    next_word = words[i+1]\n",
        "    bigram = (current_word, next_word)\n",
        "    if bigram in bigram_counts:\n",
        "        bigram_counts[bigram] += 1\n",
        "    else:\n",
        "        bigram_counts[bigram] = 1\n",
        "\n",
        "# Calculate bigram probabilities\n",
        "num_bigrams = sum(bigram_counts.values())\n",
        "bigram_probs = {bigram: count / num_bigrams for bigram, count in bigram_counts.items()}\n",
        "\n",
        "# Sample a sentence of length 10 from the bigram distribution\n",
        "start_word = random.choice(words)\n",
        "sampled_sentence = [start_word]\n",
        "for i in range(10):\n",
        "    current_word = sampled_sentence[-1]\n",
        "    possible_bigrams = [bigram for bigram in bigram_probs.keys() if bigram[0] == current_word]\n",
        "    if len(possible_bigrams) > 0:\n",
        "        sampled_bigram = random.choices(possible_bigrams, [bigram_probs[bigram] for bigram in possible_bigrams])[0]\n",
        "        next_word = sampled_bigram[1]\n",
        "    else:\n",
        "        next_word = random.choice(words)\n",
        "    sampled_sentence.append(next_word)\n",
        "\n",
        "# Print out the sampled sentence\n",
        "print(\"Sampled sentence:\")\n",
        "print(\" \".join(sampled_sentence))\n"
      ],
      "metadata": {
        "id": "083NSk6wGxwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigram language model\n",
        "\n",
        "Now let's do trigram language model where a trigram model considers the probability of a word based on the occurrence of its two preceding words in the sentence.\n",
        "\n",
        "It is very similar to the bigram language model and is an extension. It considers preceding two word context.\n"
      ],
      "metadata": {
        "id": "1zqsHruqIawM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate trigram counts\n",
        "trigram_counts = {}\n",
        "for i in range(len(words)-2):\n",
        "    current_word = words[i]\n",
        "    next_word = words[i+1]\n",
        "    third_word = words[i+2]\n",
        "    trigram = (current_word, next_word, third_word)\n",
        "    if trigram in trigram_counts:\n",
        "        trigram_counts[trigram] += 1\n",
        "    else:\n",
        "        trigram_counts[trigram] = 1\n",
        "\n",
        "# Calculate trigram probabilities\n",
        "num_trigrams = sum(trigram_counts.values())\n",
        "trigram_probs = {trigram: count / num_trigrams for trigram, count in trigram_counts.items()}\n",
        "\n",
        "# Sample a sentence of length 10 from the trigram distribution\n",
        "start_bigram = random.choices([(words[i], words[i+1]) for i in range(len(words)-1)], k=1)[0]\n",
        "sampled_sentence = [start_bigram[0], start_bigram[1]]\n",
        "for i in range(10):\n",
        "    current_bigram = (sampled_sentence[-2], sampled_sentence[-1])\n",
        "    possible_trigrams = [trigram for trigram in trigram_probs.keys() if trigram[:2] == current_bigram]\n",
        "    if len(possible_trigrams) > 0:\n",
        "        sampled_trigram = random.choices(possible_trigrams, [trigram_probs[trigram] for trigram in possible_trigrams])[0]\n",
        "        next_word = sampled_trigram[2]\n",
        "    else:\n",
        "        next_word = random.choice(words)\n",
        "    sampled_sentence.append(next_word)\n",
        "\n",
        "# Print out the sampled sentence\n",
        "print(\"Sampled sentence:\")\n",
        "print(\" \".join(sampled_sentence))"
      ],
      "metadata": {
        "id": "TZFuQSmfIX7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: \n",
        "\n",
        "1.   Try both the bigram and the trigram models with the larger dataset, do you see a difference in the generated samples? What is happening? Is the quality improving? Why is this happening?   \n",
        "2.   (Challenge excercise) Extend this for 4-gram language model. What does this involve? \n",
        "\n"
      ],
      "metadata": {
        "id": "NZGBz7wnJi3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating language models: Perplexity\n",
        "\n",
        "Recall that perplexity is a measure of how well a language model can predict a sequence of words. It is calculated as the inverse probability of the test set, normalized by the number of words in the test set, it is given as: $ppl(s) = 2^{-\\scriptscriptstyle{\\frac{\\log P(s)}{N}}}$,\n",
        "\n",
        " where $s$ is a sentence, $P(s)$ is the probability of the sentence according to the LM. \n",
        "\n",
        " We will define this below. "
      ],
      "metadata": {
        "id": "njOypTdDKzvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(test_set, ngram_probs):\n",
        "    # Split the test set into a list of words\n",
        "    test_words = test_set.split()\n",
        "    \n",
        "    # Calculate the perplexity of the test set\n",
        "    N = len(test_words)\n",
        "    log_prob = 0\n",
        "    for i in range(len(test_words)-(len(ngram_probs)-1)):\n",
        "        ngram = tuple(test_words[i:i+len(ngram_probs)])\n",
        "        if ngram in ngram_probs:\n",
        "            log_prob += math.log(ngram_probs[ngram])\n",
        "        else:\n",
        "            # Add probability of unknown word\n",
        "            log_prob += math.log(1 / (sum(ngram_probs.values()) + 1))\n",
        "    perplexity = math.exp(-log_prob/N)\n",
        "    \n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "TlaLslg7JM9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the perplexity of a test set using a trigram language model. The test set is defined as the string \"this article is from Wikipedia\". Remember, lower the perplexity -- better the quality of the LM (when trained on language related data). \n",
        "\n",
        "Another way to think of perplexity is essentially asking to what extent is the LM surprised with a given sample of data. \n"
      ],
      "metadata": {
        "id": "p7o-OSTcMs_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the test set as a string\n",
        "test_set = \"this article is from Wikipedia\"\n",
        "\n",
        "# Calculate perplexity of test set using the trigram language model\n",
        "trigram_perplexity = calculate_perplexity(test_set, trigram_probs)\n",
        "print(\"Trigram perplexity:\", trigram_perplexity)\n",
        "\n"
      ],
      "metadata": {
        "id": "-F4O-tLyJXVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO\n",
        "\n",
        "1. Can you compare the perplexity from each of the LMs that we have implemented? \n",
        "\n",
        "2. If you have constructed 4-gram language model, please evaluate the perplexity of the model. \n",
        "\n",
        "3. Can you go on in this way to {5, 6, 7, 9}-grams? Where is the bottleneck? What would extending the context window do? "
      ],
      "metadata": {
        "id": "eMjbB_T4M89w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HL_bJW87OWFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}