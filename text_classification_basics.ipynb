{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvX7+hHY2bTmiyAeYnfJYu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/text_classification_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification (fundamentals)"
      ],
      "metadata": {
        "id": "FzSf55PaJHS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = \"Pranava Madhyastha\" \n",
        "__version__ = \"INM434/IN3045 City, University of London, Spring 2023\""
      ],
      "metadata": {
        "id": "dUQGLUnlJSuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents\n",
        "\n",
        "\n",
        "1. [Rule based classifier](#rulebased)\n",
        "2. [Simple logistic regression based classifier](#logreg)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "nHJ1m_T6JZUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rule based classifier\n",
        "\n",
        "Let us begin with rule based classifier. In order to build the classifier, we have to first perform some basic tokenisation. We will assume that most of the rules in this toy classifier are based on word based information. To make our task easy, let us only consider a small set of words that relate to the sentiments. \n",
        "\n",
        "In our classifier, we will first define a set of potential cases, there are two cases here: positive words (for detecting positive sentiment) and negative words (for detecting negative sentiment). \n",
        "\n",
        "We will then write a very simple function that just *checks* if a certain word from one of these bags are present, then predicts the potential sentiment of the sentence.  "
      ],
      "metadata": {
        "id": "lytRfc7XJ4bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJbZ1oGj_chw"
      },
      "outputs": [],
      "source": [
        "positive_words = ['good', 'great', 'love', 'excellent', 'fantastic', 'amazing']\n",
        "negative_words = ['bad', 'terrible', 'hate', 'awful', 'disappointing']\n",
        "\n",
        "def classify_sentiment(sentence):\n",
        "  sentiment = 'neutral'\n",
        "  words = sentence.lower().split()\n",
        "  for word in words:\n",
        "    if word in positive_words:\n",
        "      sentiment = 'positive'\n",
        "      break\n",
        "    elif word in negative_words:\n",
        "      sentiment = 'negative'\n",
        "      break\n",
        "  return sentiment\n",
        "\n",
        "sentences = ['I love this movie', 'This is a terrible film', 'The acting was fantastic', 'The movie was not disappointing']\n",
        "for sentence in sentences:\n",
        "  print(f'{sentence}: {classify_sentiment(sentence)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Identify the potential problems with this approach.\n",
        "\n",
        "### TODO: What happened with last example? Why is this happening? \n",
        "\n",
        "### TODO: Can you fix this? "
      ],
      "metadata": {
        "id": "9BlD9szdLBAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text classification using a logistic regression. \n",
        "\n",
        "Logistic Regression is a type of a linear model commonly used for binary classification problems. It is oftentimes used to model the probability of an event occurring, given a set of input *features*. \n",
        "\n",
        "As a linear model, logistic regression finds a linear boundary that separates the data into two *classes*.\n",
        "\n",
        "Typically, in logistic regression, the dot product of weights and features produce a score for each sample. The score is then transformed using the logistic function (also called the sigmoid function) to produce a probability between 0 and 1. The probability obtained can be interpreted as the likelihood that a given input belongs to the positive class.\n",
        "\n",
        "\n",
        "\n",
        "In the code below, we begin with the main function that extracts features. \n",
        "\n",
        "### TODO: What types of features are these? \n",
        "\n",
        "After the extraction of the features, we have a few helper functions: \n",
        "\n",
        "\n",
        "\n",
        "1. `sigmoid`: This maps a real-valued number to the range of 0 to 1.\n",
        "2. `binary_cross_entropy`: This function calculates the binary cross-entropy loss between the true labels and the predicted probabilities. \n",
        "3. `predict`: performs prediction\n",
        "4. `evaluate`: This function takes a set of weights, a dataset, and a vocabulary and returns the accuracy of the model on the dataset.\n",
        "5. `train`: This function trains the logistic regression model. It first extracts the features of the dataset using the extract_features function, then initializes the weights to be zero, and trains the model for a specified number of epochs by updating the weights using the gradient of the binary cross-entropy loss. \n",
        "\n"
      ],
      "metadata": {
        "id": "RTrniKY4MTdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the toy dataset\n",
        "data = [(\"I love this movie\", 1), \n",
        "        (\"This is a great film\", 1), \n",
        "        (\"I hated this movie\", 0), \n",
        "        (\"This film was terrible\", 0), \n",
        "        (\"I'm not a fan of this movie\", 0), \n",
        "        (\"This movie is fantastic\", 1), \n",
        "        (\"I love going to the movies\", 1), \n",
        "        (\"This movie is a disaster\", 0), \n",
        "        (\"I enjoyed watching this film\", 1), \n",
        "        (\"This movie was a waste of time\", 0)]\n",
        "\n",
        "# Function to extract features from the text data\n",
        "def extract_features(data, vocabulary):\n",
        "    features = []\n",
        "    for text, label in data:\n",
        "        words = text.lower().split()\n",
        "        feature = [0] * len(vocabulary)\n",
        "        for word in words:\n",
        "            if word in vocabulary:\n",
        "                feature[vocabulary.index(word)] = 1\n",
        "        features.append(feature)\n",
        "    return np.array(features)\n",
        "\n",
        "def predict(weights, features):\n",
        "    z = np.dot(features, weights)\n",
        "    return sigmoid(z)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def binary_cross_entropy(labels, predictions):\n",
        "    return -np.mean(np.array(labels) * np.log(predictions) + (1 - np.array(labels)) * np.log(1 - predictions))\n",
        "\n",
        "def evaluate(weights, data, vocabulary):\n",
        "    features = extract_features(data, vocabulary)\n",
        "    labels = [label for _, label in data]\n",
        "    predictions = predict(weights, features)\n",
        "    predictions = [1 if p >= 0.5 else 0 for p in predictions]\n",
        "    accuracy = np.mean(np.array(predictions) == np.array(labels))\n",
        "    return accuracy\n",
        "\n",
        "def train(data, vocabulary, learning_rate=0.1, epochs=1000):\n",
        "    features = extract_features(data, vocabulary)\n",
        "    labels = [label for _, label in data]\n",
        "    weights = np.zeros(len(vocabulary))\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "    for epoch in range(epochs):\n",
        "        predictions = predict(weights, features)\n",
        "        gradient = np.dot(features.T, predictions - labels)\n",
        "        weights = weights - learning_rate * gradient\n",
        "        loss = binary_cross_entropy(labels, predictions)\n",
        "        loss_history.append(loss)\n",
        "        accuracy = evaluate(weights, data, vocabulary)\n",
        "        accuracy_history.append(accuracy)\n",
        "    return weights, loss_history, accuracy_history\n",
        "\n",
        "# Create the vocabulary\n",
        "text = \" \".join([text for text, label in data])\n",
        "vocabulary = set(text.lower().split())\n",
        "vocabulary = list(vocabulary)\n",
        "\n",
        "# Train the classifier\n",
        "weights, loss_history, accuracy_history = train(data, vocabulary)\n",
        "plt.plot(loss_history, label='Loss')\n",
        "plt.show()\n",
        "\n",
        "test_data = [\n",
        "    (\"good one\", 1),\n",
        "    (\"not good\", 0),\n",
        "    (\"did not like\", 0),\n",
        "    (\"horrible movie\", 0),\n",
        "    (\"love it\", 1),\n",
        "     (\"not a bad movie\", 1),\n",
        "]\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = evaluate(weights, test_data, vocabulary)\n",
        "print(accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "BWsPTzkrAUo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Alter the weight initialisation to random. (use numpy's build in random function). What happens now to the training curve? \n",
        "\n",
        "### TODO advanced: This is a binary classifier. How would you extend it to a multi-class classifier? Can you extend the code? What components would have to change? "
      ],
      "metadata": {
        "id": "xlDt9f7xU4T2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using trusted libraries\n",
        "\n",
        "Ideally, we would reuse trusted libraries to perform NLP. Some of the commonly recommended ones are: sklearn, nltk, spacy, torchtext, huggingface: transformers, datasets. \n",
        "\n",
        "In code below, we will attempt to implement logistic regression using sklearn. "
      ],
      "metadata": {
        "id": "iW9IN299YlUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Create the features and labels\n",
        "texts = [text for text, label in data]\n",
        "labels = [label for _, label in data]\n",
        "\n",
        "# Extract the features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "features = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Train the classifier\n",
        "model = LogisticRegression()\n",
        "model.fit(features, labels)\n",
        "\n",
        "# Evaluate the classifier on test data\n",
        "\n",
        "test_texts = [text for text, label in test_data]\n",
        "test_labels = [label for _, label in test_data]\n",
        "test_features = vectorizer.transform(test_texts)\n",
        "test_predictions = model.predict(test_features)\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "MyeETSUQVSC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: What is the test-accuracy? What is happening? Compute the train accuracy. \n",
        "\n",
        "### TODO: What is the vocabulary? Are the features different? Perform error analysis. \n",
        "### TODO (extra): Match the outputs frm both the models. "
      ],
      "metadata": {
        "id": "tNuv0we_aFWd"
      }
    }
  ]
}