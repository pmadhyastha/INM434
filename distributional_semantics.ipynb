{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP0PBYFu9lJqb6NB5Khp+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/distributional_semantics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributional models, vector space representations and word embeddings"
      ],
      "metadata": {
        "id": "eaIGgZuxgrdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = \"Pranava Madhyastha\"\n",
        "__version__ = \"INM434/IN3045 City, University of London, Spring 2024\""
      ],
      "metadata": {
        "id": "Mr5YS1tWgtMS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: english wiki corpus!\n",
        "\n",
        "We will begin by downloading a large wikipedia corpus and perform some cleaning and text normalisation."
      ],
      "metadata": {
        "id": "w4EVa0xggybl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import re\n",
        "\n",
        "# Download the corpus\n",
        "url = \"http://mattmahoney.net/dc/enwik8.zip\"\n",
        "urllib.request.urlretrieve(url, \"enwik8.zip\")\n",
        "\n",
        "# Extract the corpus and clean it\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('enwik8.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "with open('enwik8', 'r', encoding='utf-8') as f_in, open('enwik8_clean', 'w', encoding='utf-8') as f_out:\n",
        "    for line in f_in:\n",
        "        # Strip off HTML tags\n",
        "        line = re.sub(r'<.*?>', '', line)\n",
        "        # Normalize the text\n",
        "        line = line.lower()\n",
        "        # Write the cleaned line to the output file\n",
        "        f_out.write(line)\n",
        "\n",
        "# Tokenize the cleaned corpus\n",
        "with open('enwik8_clean', 'r', encoding='utf-8') as f:\n",
        "    corpus = f.read()"
      ],
      "metadata": {
        "id": "8NaPBH05gwSF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code downloads an old version of English wikipedia corpus.\n",
        "\n",
        "### TODO:\n",
        "\n",
        "1.   How are we normalising text? Can you print out a part of the corpus and see what is being done?"
      ],
      "metadata": {
        "id": "VXOBjpWUg5Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UtHoyATeg-LX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector space represenation of words using co-occurrence information.\n",
        "\n",
        "We will now write code for getting out first vector space model by building very simple word co-occurrence dictionary (matrix and dictionaries and interchangeably used in this lab session).\n",
        "\n",
        "\n",
        "The program reads in a corpus of text stored in the file called 'enwik8_clean' (which we obtained using the process above). The program then tokenizes it using NLTK library. It then creates a vocabulary of *unique words* in the corpus and counts the number of occurrences of each word using a defaultdict object.\n",
        "\n",
        "Next, it counts the co-occurrences of words within a fixed window size of three words and stores the results in a nested defaultdict.\n",
        "\n",
        "The program then creates a vector space representation for each word by iterating through the vocabulary and creating a vector of co-occurrence counts with all other words in the vocabulary.\n",
        "\n",
        "The resulting vectors are stored in a dictionary object named 'vectors'. The code also filters out stop words from the corpus before counting co-occurrences."
      ],
      "metadata": {
        "id": "dvtVBP0Gg-Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Constants\n",
        "WINDOW_SIZE = 3\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Read in corpus\n",
        "with open('enwik8_clean', 'r', encoding='utf-8') as f:\n",
        "    filesize = os.path.getsize('enwik8_clean')\n",
        "    lines_limit = 1000\n",
        "    lines = []\n",
        "    line = 0\n",
        "    while line < lines_limit:\n",
        "        l = f.readline()\n",
        "        lines.append(l)\n",
        "        line += 1\n",
        "    corpus = ''.join(lines)\n",
        "\n",
        "# Create vocabulary\n",
        "corpus = word_tokenize(corpus)\n",
        "vocab = set(corpus)\n",
        "\n",
        "print(len(vocab))\n",
        "\n",
        "# Count occurrences of each word\n",
        "word_counts = defaultdict(int)\n",
        "for word in corpus:\n",
        "    word_counts[word] += 1\n",
        "\n",
        "# Count co-occurrences of words within a window\n",
        "context_counts = defaultdict(lambda: defaultdict(int))\n",
        "for i in range(len(corpus)):\n",
        "    if corpus[i] in STOPWORDS:\n",
        "        continue\n",
        "    for j in range(i - WINDOW_SIZE, i + WINDOW_SIZE + 1):\n",
        "        if j < 0 or j >= len(corpus) or i == j or corpus[j] in STOPWORDS:\n",
        "            continue\n",
        "        context_counts[corpus[i]][corpus[j]] += 1\n",
        "\n",
        "# Create vector space representation for each word\n",
        "vectors = {}\n",
        "for word in vocab:\n",
        "    vector = [context_counts[word][w] for w in vocab]\n",
        "    vectors[word] = vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNmQ9RFsg1nK",
        "outputId": "db5dc8af-8643-4d54-810b-2277936249da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO:\n",
        "\n",
        "1. How many lines of data are we considering the code?\n",
        "2. \"vectors\" is a dictionary, can you print the vocabulary?\n",
        "3. What is the dimensionality of the word vector?\n",
        "4. What are we storing in each one of the arrays corresponding to each word?\n",
        "5. Consider the word \"peaceful\", what is the colsum of this word? What does colsum signify here?\n",
        "6. Feel free to increase the number of lines, there is a point at which it is going to occupy a very large amount of memory and \"colab\" will stop functioning. What is causing this?\n",
        "7. What is a way to mitigate the memory explosion?  "
      ],
      "metadata": {
        "id": "wu3Uw_uEhFan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word similarity using metrics\n",
        "\n",
        "We will now play with vector similarity. We want to essentially understand how we can obtain the most similar and the least similar words. We will use distance metrics to operationalise this."
      ],
      "metadata": {
        "id": "omCkL8N2hOES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def top_n_words(vectors, n, distance_function):\n",
        "    distances = {}\n",
        "    for word, vector in vectors.items():\n",
        "        distance = distance_function(vector)\n",
        "        distances[word] = distance\n",
        "    sorted_distances = sorted(distances.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_distances[:n]\n",
        "\n",
        "# Example dictionary of vectors\n",
        "vectors = {\n",
        "    'apple': np.array([1, 2, 3]),\n",
        "    'banana': np.array([4, 5, 6]),\n",
        "    'orange': np.array([7, 8, 9]),\n",
        "    'grape': np.array([10, 11, 12])\n",
        "}\n",
        "\n",
        "# Euclidean distance function\n",
        "def euclidean_distance(vector):\n",
        "    return np.linalg.norm(vector)\n",
        "\n",
        "# Manhattan distance function\n",
        "def manhattan_distance(vector):\n",
        "    return np.sum(np.abs(vector))\n",
        "\n",
        "# Top-n words using Euclidean distance\n",
        "top_n_euclidean = top_n_words(vectors, 2, euclidean_distance)\n",
        "print('Top 2 words using Euclidean distance:', top_n_euclidean)\n",
        "\n",
        "# Top-n words using Manhattan distance\n",
        "top_n_manhattan = top_n_words(vectors, 2, manhattan_distance)\n",
        "print('Top 2 words using Manhattan distance:', top_n_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-NuPVAOhCUS",
        "outputId": "0e240aef-eabc-4bdd-c68f-282e17691a2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 2 words using Euclidean distance: [('grape', 19.1049731745428), ('orange', 13.92838827718412)]\n",
            "Top 2 words using Manhattan distance: [('grape', 33), ('orange', 24)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO:\n",
        "\n",
        "1. Can you extend the code for cosine similarity and cosine distance?\n",
        "2. Have a look at https://docs.scipy.org/doc/scipy/reference/spatial.distance.html for additional distance metrics. Try different distance functions and see how top-2 words change!\n",
        "3. We are playing with toy data for this code, can you instead use the vectors from the example above and obtain the top-2 words for a few words in the vocab? (say start with the word \"peaceful\").\n",
        "4. Can you try playing with a few of reweighting techniques? See how distances change?"
      ],
      "metadata": {
        "id": "9XHfm20UhZ3e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "frL5JjEKhV2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}