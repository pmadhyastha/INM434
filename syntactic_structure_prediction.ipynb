{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsipALw2r2E6Xh1es1AOZ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/syntactic_structure_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Syntax and Syntactic Structure Prediction"
      ],
      "metadata": {
        "id": "DVoYGY3vsfaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "3rzijlnVdsnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing language using CFG\n",
        "\n",
        "In the code below, we will implement the CYK algorithm (this is a toy implementation). \n",
        "\n",
        "The Grammar class is the primary class below which contains several variables and functions that will be used to load and parse a context-free grammar, represented by a list containing production rules (we are giving this, later).\n",
        "\n",
        "\n",
        "Notice that we have a custom dictlist class. The grammar_rules variable is a Dictlist object that stores the production rules of the grammar, where the keys are the right-hand side of the production and the values are the left-hand side. \n",
        "\n",
        "The parse_table variable is initialized as None, and will later be used to store the parse table created during the CYK parsing algorithm.\n",
        "\n",
        "The Grammar class constructs the grammar_rules dictionary. The right-hand side of the production is used as the key in the grammar_rules dictionary, and the left-hand side is stored as the corresponding value.\n",
        "\n",
        "The apply_rules function takes a string as input and returns the left-hand side of the production rule that matches the right-hand side of the input string. \n",
        "\n",
        "The CYK parsing algorithm consists of two nested loops. The outer loop iterates over the length of the input sentence (i.e., the number of words in the sentence), and the inner loop iterates over the starting position of the substring being considered. For example, if the sentence is \"the cat sat on the mat\", the inner loop would start with \"the\", then \"cat\", then \"sat\", and so on.\n",
        "\n",
        "The first loop, which is over the length of the substring being considered, starts with 2 and goes up to the length of the sentence. The second loop, which is over the starting position of the substring being considered, starts with 1 and goes up to the length of the sentence minus the length of the substring plus 1. For each combination of substring length and starting position, the function then iterates over all possible ways of splitting the substring into two parts, represented by the variable p. For example, if the substring being considered is \"the cat sat\", p would be 1 (for splitting the substring into \"the\" and \"cat sat\"), 2 (for splitting the substring into \"the cat\" and \"sat\"), and 3 (for splitting the substring into \"the cat sat\" and \"\").\n",
        "\n",
        "For each combination of substring length, starting position, and split point, the function retrieves the set of rules that can generate the left-hand side of the substring to the left of the split point and the set of rules that can generate the right-hand side of the substring to the right of the split point. It then iterates over all possible pairs of rules from the two sets and checks whether there is a production rule in the grammar that generates the pair of rules.\n",
        "\n"
      ],
      "metadata": {
        "id": "2iySwcn_o-H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "class DictList(dict):\n",
        "    \n",
        "    def __setitem__(self, key, value):\n",
        "        try:\n",
        "            self[key]\n",
        "        except KeyError:\n",
        "            super(DictList, self).__setitem__(key, [])\n",
        "        self[key].append(value)\n",
        "\n",
        "\n",
        "class ProductionRule:\n",
        "    \n",
        "    def __init__(self, result, p1=None, p2=None):\n",
        "        self.result = result\n",
        "        self.p1 = p1\n",
        "        self.p2 = p2\n",
        "    \n",
        "    @property\n",
        "    def type(self):\n",
        "        return self.result\n",
        "    \n",
        "    @property\n",
        "    def left_child(self):\n",
        "        return self.p1\n",
        "    \n",
        "    @property\n",
        "    def right_child(self):\n",
        "        return self.p2\n",
        "\n",
        "\n",
        "class Cell:\n",
        "    \n",
        "    def __init__(self, productions=None):\n",
        "        self.productions = productions or []\n",
        "            \n",
        "    def add_production(self, result, p1=None, p2=None):\n",
        "        self.productions.append(ProductionRule(result, p1, p2))\n",
        "    \n",
        "    def set_productions(self, productions):\n",
        "        self.productions = productions\n",
        "    \n",
        "    @property\n",
        "    def types(self):\n",
        "        return [p.type for p in self.productions]\n",
        "    \n",
        "    @property\n",
        "    def rules(self):\n",
        "        return self.productions\n",
        "\n",
        "\n",
        "class Grammar:\n",
        "    \n",
        "    def __init__(self, elist):\n",
        "        self.grammar_rules = DictList()\n",
        "        self.parse_table = None\n",
        "        self.length = 0\n",
        "        for line in elist:\n",
        "            a, b = line.split(\"->\")\n",
        "            self.grammar_rules[b.rstrip().strip()] = a.rstrip().strip()\n",
        "        \n",
        "        print('')\n",
        "        print('Grammar loaded. Rules read:')\n",
        "        self.print_rules()\n",
        "        print('')\n",
        "    \n",
        "    def print_rules(self):\n",
        "        for r in self.grammar_rules:\n",
        "            for p in self.grammar_rules[r]:\n",
        "                print(f\"{p} --> {r}\")\n",
        "        \n",
        "    def apply_rules(self, t):\n",
        "        return self.grammar_rules.get(t, None)\n",
        "            \n",
        "    def parse(self, sentence):\n",
        "        self.number_of_trees = 0\n",
        "        self.tokens = sentence.split()\n",
        "        self.length = len(self.tokens)\n",
        "        self.parse_table = [[Cell() for _ in range(self.length - y)] for y in range(self.length)]\n",
        "        \n",
        "        # Process the first line\n",
        "        for x, t in enumerate(self.tokens):\n",
        "            r = self.apply_rules(t)\n",
        "            if r is None:\n",
        "                raise ValueError(f\"The word {t} is not in the grammar\")\n",
        "            for w in r:\n",
        "                self.parse_table[0][x].add_production(w, ProductionRule(t))\n",
        "        \n",
        "        # Run CYK-Parser\n",
        "        for l in range(2, self.length+1):\n",
        "            for s in range(1, self.length-l+2):\n",
        "                for p in range(1, l-1+1):\n",
        "                    t1 = self.parse_table[p-1][s-1].rules\n",
        "                    t2 = self.parse_table[l-p-1][s+p-1].rules\n",
        "                    for a in t1:\n",
        "                        for b in t2:\n",
        "                            r = self.apply_rules(f\"{a.type} {b.type}\")\n",
        "                            if r is not None:\n",
        "                                for w in r:\n",
        "                                    print(f\"This rule is being applied: {w}[{l},{s}] --> {a.type}[{p},{s}] {b.type}[{l-p},{s+p}]\")\n",
        "                                    self.parse_table[l-1][s-1].add_production(w, a, b)\n",
        "                               \n",
        "        self.number_of_trees = len(self.parse_table[self.length-1][0].types)\n",
        "        if self.number_of_trees > 0:\n",
        "            print(f\"Number of possible trees: {self.number_of_trees}\")\n",
        "        else:\n",
        "            print(f\"The sentence seems incorrect\")\n",
        "        \n",
        "    def get_trees(self):\n",
        "        return self.parse_table[self.length-1][0].productions\n",
        "\n",
        "    def print_parse_table(self):\n",
        "        lines = []\n",
        "\n",
        "        for row in reversed(self.parse_table):\n",
        "            l = []\n",
        "            for cell in row:\n",
        "                l.append(cell.types)\n",
        "            lines.append(l)\n",
        "        \n",
        "        lines.append(self.tokens)\n",
        "        print(tabulate(lines))"
      ],
      "metadata": {
        "id": "ZndjjmpYeHnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = ['S -> NP VP', 'PP -> P NP', 'VP -> V NP', 'VP -> VP PP', 'NP-> NP PP', 'NP -> pizza', 'NP -> I', 'NP -> eat', 'NP-> pineapple', 'P -> with', 'V -> ate']\n"
      ],
      "metadata": {
        "id": "FxRqFb3siHwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = Grammar(example)\n",
        "g.parse('I eat pizza with pineapple')\n",
        "g.print_parse_table()"
      ],
      "metadata": {
        "id": "rp1Ii6rJiFdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Todo: \n",
        "- Give another example in the same format (this is known as the Chomsky Normal Form) \n",
        "- Try different sentences. "
      ],
      "metadata": {
        "id": "aU4OUADNsq3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring dependency parsing\n",
        "\n",
        "In the following, we will try and navigate dependency parse tree of sentences. We will however rely on a trained model for extracting parses. We will use a popular NLP library called `spacy`. \n",
        "\n",
        "We will first download the library.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tw7gt8abQCUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "mPAxMXmjQpEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now import `spacy` library and load the pre-trained English model \"en_core_web_sm\" (please refer to spacy documentation for further information about the library). "
      ],
      "metadata": {
        "id": "1NWuirHxQ-dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "pipeline = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "6jqWQwL3Q560"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\"\n",
        "\n",
        "# from wikipedia! "
      ],
      "metadata": {
        "id": "Asz9_3NmRNGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now pass the sentence through `spacy` pipeline, which runs many things. It also generates the dependency parse."
      ],
      "metadata": {
        "id": "B8ai7SKiUV0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_piped_output = pipeline(sample_text)\n"
      ],
      "metadata": {
        "id": "7gk8oJ-LRV-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are really interested in visualising the dependency parse. The code below helps us visualise the dependency parse: "
      ],
      "metadata": {
        "id": "-3Scd4wxUj8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(sample_piped_output, style=\"dep\", jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "-gNiCNQCTvqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isn't that sentence a tad long and difficult to process in our head? \n",
        "\n",
        "\n",
        "## TODO: \n",
        "- Is there a mistake in the parse tree? \n",
        "- Try your own set of sentences and visualise the parse tree. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHmqIxRcUq09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now print the children of \"is\".  Is this correct? Does it make sense? "
      ],
      "metadata": {
        "id": "htKaOQ9YV5om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([token.text for token in sample_piped_output[9].children])\n",
        "\n"
      ],
      "metadata": {
        "id": "G6VKHcaHSP73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now print subtree for the word \"is\""
      ],
      "metadata": {
        "id": "hRVC12sSWgwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (list(sample_piped_output[9].subtree))"
      ],
      "metadata": {
        "id": "GIU8VT9jSzTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go through each of the word and print out the part of speech tag, the dependency relation for each of the word in the sentence. "
      ],
      "metadata": {
        "id": "JB4uSL8MXQ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in sample_piped_output:\n",
        "    print(\"%s: %s %s\" % (word, word.pos_, word.dep_))"
      ],
      "metadata": {
        "id": "WYoVNGmGWMGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: \n",
        "- Can you get all the subjects and objects in this sentence? \n",
        "- (to know more about dependency labels please visit: https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf)"
      ],
      "metadata": {
        "id": "x9xQIkIQXZ7s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mHAi32MPm9sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}