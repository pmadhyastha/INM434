{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlBrYsxNWf4oacXVQr1r/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/text_classification_advanced_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Text processing with linear and non-linear models\n",
        "\n"
      ],
      "metadata": {
        "id": "tFCRVxAcueH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = \"Pranava Madhyastha\"\n",
        "__version__ = \"INM434/IN3045 City, University of London, Spring 2025\""
      ],
      "metadata": {
        "id": "upBFFf0wukHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importance of features\n",
        "\n",
        "We will begin with experimets on features. But before we begin, let us download datasets library from huggingface which gives us access to a large number of datasets.\n",
        "\n",
        "We will locally install the library (this is temporary and will disappear as soon the session is restarted)."
      ],
      "metadata": {
        "id": "6Kbca122lNaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preparation\n",
        "!pip install datasets # we are installing huggingface datasets"
      ],
      "metadata": {
        "id": "i1EXjybwlaNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset and experimenting with features\n",
        "\n",
        "For the following set of worked out examples we will work with sentiment portion of the tweet_eval dataset. Read the documentation for the dataset here: https://huggingface.co/datasets/tweet_eval.\n",
        "\n",
        "The function load_dataset automatically creates the pre-designed split of train and test. This helps in proper comparison of the models.\n",
        "\n",
        "We will first begin with linear model (logistic regression) and play with different types of features on the same dataset.\n",
        "\n",
        "Please attempt the TODOs mentioned in the code below."
      ],
      "metadata": {
        "id": "wvTwwfN7KSCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UZCF1A9-j9j6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Custom feature extraction function\n",
        "\n",
        "# TODO - Experiments with features:\n",
        "#   1. Experiment with different ngrams\n",
        "#   2. Read the documentation for CountVectorizer - see what happens with the analyzer argument?\n",
        "#   3. Try changing max_df and min_df\n",
        "#   4. Try changing max_features\n",
        "#   5. What happens when you binarise the features?\n",
        "#   6. Experiment stopwords. See the impact of stop words.\n",
        "#   7. Experiment with lower casing tokens\n",
        "#   8. Advanced - can you write a new preprocessor function - this goes inside CountVectorizer.\n",
        "\n",
        "def custom_feature_extraction(text, ngrams=3, compute=0, vectorizer=None):\n",
        "    if compute:\n",
        "    # Extract unigrams, bigrams, and trigrams\n",
        "        vectorizer = CountVectorizer(ngram_range=(1, ngrams))\n",
        "        features = vectorizer.fit_transform(text)\n",
        "        return features, vectorizer\n",
        "    else:\n",
        "        features = vectorizer.transform(text)\n",
        "        return features\n",
        "\n",
        "\n",
        "# Load sentiment analysis dataset from Hugging Face\n",
        "dataset = load_dataset('tweet_eval', 'sentiment')\n",
        "\n",
        "# Extract input and output data\n",
        "X = dataset['train']['text']\n",
        "y = dataset['train']['label']\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "\n",
        "# Further split training set into smaller training set and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=123)\n",
        "\n",
        "\n",
        "# Extract features using custom feature extraction function\n",
        "X_train_feats, feat_extractor = custom_feature_extraction(X_train, compute=1)\n",
        "X_val = custom_feature_extraction(X_val, compute=0, vectorizer=feat_extractor)\n",
        "X_test = custom_feature_extraction(X_test, compute=0, vectorizer=feat_extractor)\n",
        "\n",
        "# Train linear model with logistic regression\n",
        "clf = LogisticRegression(verbose=True)\n",
        "clf.fit(X_train_feats, y_train)\n",
        "\n",
        "# Evaluate model on validation set\n",
        "val_score = clf.score(X_val, y_val)\n",
        "print('Validation score:', val_score)\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_score = clf.score(X_test, y_test)\n",
        "print('Test score:', test_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Solutions:\n",
        "\n",
        "1. Experiment with different n-grams: modify `ngrams` object with 1, 2, 4 and observe the changes.\n",
        "2. Experiment with different `analyzers`: modify `custom_feature_extraction` to allow for custom analyzers, you will see a description of the types of analyzers. In the context of scikit-learn's CountVectorizer, an \"analyzer\" is a parameter that determines how the text is broken down into features. It controls the type of features that are extracted from the text data. CountVectorizer offers three main analyzer options: `word`,`char`,`char_wb` -- Please refer to this, experiment and see how things change.\n",
        "3. again from documentation, in scikit-learn's CountVectorizer, `max_df` is a parameter that helps filter out terms that appear too frequently in the document collection. It stands for \"maximum document frequency.\"\n",
        "4. `max_features` requires similar modifications as for the above two. Again - read the documentation - see that it limits vocabulary size to most frequent terms.\n",
        "5. setting `binary=True` requires similar modifications -- it converts counts to binary presence/absence features.\n",
        "6. Stopword filtering removes stop words -- stop words recurr everywhere!\n",
        "7. This is about case sensitivity - this will require you to modify the main function (and hence the sub functions) to allow for setting lower and uppper case parametrisations.\n",
        "8. Here is one that uses regular expressions from Lab 1:\n",
        "\n",
        "```\n",
        "def custom_preprocessor(text):\n",
        "    \"\"\"\n",
        "    Custom preprocessor to clean tweets: removes URLs, removes user mentions (@username). removes special characters but keeps emoticons, normalizes whitespace --- why am I doing all this -- because I saw a few samples.\n",
        "    \"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    \n",
        "    # Remove user mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    \n",
        "    # Keep emoticons but remove other special characters\n",
        "    # The negative lookahead (?![\\w\\s]) ensures we don't remove emoticons\n",
        "    text = re.sub(r'[^\\w\\s](?![\\w\\s])', ' ', text)\n",
        "    \n",
        "    # Normalize whitespace (replace multiple with single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "  ```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uzOI-mdKans5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot the learning curves\n",
        "\n",
        "Let us now see how the learning curves vary with different design choices of the features.\n"
      ],
      "metadata": {
        "id": "vxTTe9WWHnbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "\n",
        "X_train_ngrams_1, feat_extractor = custom_feature_extraction(X_train, ngrams=1, compute=1)\n",
        "X_train_ngrams_2, feat_extractor = custom_feature_extraction(X_train, ngrams=2, compute=1)\n",
        "X_train_ngrams_3, feat_extractor = custom_feature_extraction(X_train, ngrams=3, compute=1)\n",
        "X_train_ngrams_4, feat_extractor = custom_feature_extraction(X_train, ngrams=4, compute=1)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate learning curve\n",
        "train_sizes_1, train_scores_1, cv_scores_1 = learning_curve(clf, X_train_ngrams_1, y_train, cv=5, train_sizes=[0.1, 0.3, 0.5, 0.7, 0.9])\n",
        "#train_sizes_2, train_scores_2, cv_scores_2 = learning_curve(clf, X_train_ngrams_2, y_train, cv=5, train_sizes=[0.1, 0.3, 0.5, 0.7, 0.9])\n",
        "#train_sizes_3, train_scores_3, cv_scores_3 = learning_curve(clf, X_train_ngrams_3, y_train_sample, cv=5, train_sizes=[0.1, 0.3, 0.5, 0.7, 0.9])\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(train_sizes_1, cv_scores_1.mean(axis=1), label='Cross-validation score for clf-1')\n",
        "#plt.plot(train_sizes_2, cv_scores_2.mean(axis=1), label='Cross-validation score for clf-2')\n",
        "\n",
        "\n",
        "plt.title('Learning curve')\n",
        "plt.xlabel('Training set size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y7jdR-2vmVrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "1. Plot all the variants of different feature designs.\n",
        "2. Experiment with SVM inplace of Logistic Regression.\n",
        "3. What is the loss for SVM?\n",
        "4. Test your classifiers on a different sentiment dataset from hugginface.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "83t_Lq9CHzVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. I will let this as an exercise.\n",
        "2. for this, use the relevant libraries: `from sklearn.svm import SVC, LinearSVC` and then we will try with a simple linear SVM, only change the classifier variable from above -- `clf = LinearSVC(random_state=123, max_iter=2000, dual=\"auto\")`\n",
        "3. We saw this in the class briefly -- it is max margin loss.\n",
        "4. I leave this as an assignment -- the `datasets` library should allow you to easily experiment with.\n"
      ],
      "metadata": {
        "id": "Hvo3jKmXdiF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-linear models\n",
        "\n",
        "We will only cover multi-layer perceptron (a feed-forward model) in this lab.\n",
        "\n",
        "We will first make use of scikit-learn's MLP classifier.\n",
        "\n",
        "Let us build a simple 2 hidden layer feedforward model with ReLU non-linearity."
      ],
      "metadata": {
        "id": "2sWUkmaYI2Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets (validation splits)\n",
        "# It is always a good idea to have validation set - this is where we will tune the hyperparameters.\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(dataset['train']['text'], dataset['train']['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encode the text using CountVectorizer and calculate the mean pooling\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "train_data = vectorizer.fit_transform(train_texts)\n",
        "train_data = train_data.multiply(1 / train_data.sum(axis=1)) # what is happening here?\n",
        "\n",
        "val_data = vectorizer.transform(val_texts)\n",
        "val_data = val_data.multiply(1 / val_data.sum(axis=1))\n",
        "\n",
        "# Build the neural network model with two hidden layers\n",
        "model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', max_iter=5, verbose=True)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "model.fit(train_data, train_labels)\n",
        "train_acc = accuracy_score(train_labels, model.predict(train_data))\n",
        "val_acc = accuracy_score(val_labels, model.predict(val_data))\n",
        "\n",
        "print(train_acc, val_acc)"
      ],
      "metadata": {
        "id": "JvAgZsgqw1A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "1. Evaluate the new model on the original test set.\n",
        "2. Is it taking a long time to train? Why?\n",
        "3. Can you experiment with different features?\n",
        "4. Is this model better?\n",
        "5. What would you change? (the hyperparameters) - how are these different from \"feature combination\" based design choices?\n",
        "6. Can you perform model comparison with hypothesis testing?"
      ],
      "metadata": {
        "id": "OtKeh3ZbJSVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Here is some sample code:  \n",
        "```\n",
        "test_data = vectorizer.transform(dataset['test']['text'])\n",
        "test_data = test_data.multiply(1 / test_data.sum(axis=1))\n",
        "test_acc = accuracy_score(dataset['test']['label'], model.predict(test_data))\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "```\n",
        "2. There are many reasons, but the dimensionality is likely the biggest factor - neural networks can be slow with high-dimensional sparse text features.\n",
        "\n",
        "3. try a few different ways from the feature engineering that you did in exercise 1 and 2.\n",
        "\n",
        "4. For that you would have to compare the accuracy metrics on the same test splits across models. Consider training time and inference speed trade-offs?\n",
        "Examine the error patterns.\n",
        "\n",
        "5. You could do several things here:\n",
        "\n",
        "```\n",
        "hidden_layer_sizes - try different architectures (more/fewer layers, different neuron counts)\n",
        "activation - try 'tanh' or 'logistic' instead of 'relu'\n",
        "alpha - L2 regularization strength\n",
        "learning_rate and learning_rate_init - control how weights are updated\n",
        "batch_size - could impact convergence speed\n",
        "max_iter - definitely increase from just 5\n",
        "solver - try 'adam' or 'sgd' instead of default 'adam'\n",
        "```\n",
        "\n",
        "6. Consider checking for hypothesis testing - this will help compare the outputs of the models. If the scores are similar, test like McNemar's test, Wilcoxon signed-rank test can help. They will help establish if the results are statistically significant.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0_RZlGkTgXN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pytorch + GPU acceleration\n",
        "\n",
        "THe code below reimplements the non-linear model using pytorch. We will be using GPU, to set the environment to GPU please change the runtime type and select the runtime with standard GPU.\n",
        "\n",
        "Try going back to CPU. Increase the batch size."
      ],
      "metadata": {
        "id": "fqMVr3W-Py-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "\n",
        "# This line of code assigns the device that will be used for running of the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert the text to a one-hot encoding\n",
        "        text = self.vocab.transform([self.texts[idx]])\n",
        "        text = text.multiply(1 / text.sum(axis=1))\n",
        "\n",
        "        # Calculate the mean pooling of the one-hot encoding\n",
        "        text = torch.tensor(text.todense()[0])\n",
        "        text = torch.mean(text, dim=0)\n",
        "\n",
        "        # Convert the label to a tensor\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "\n",
        "        return text, label\n",
        "\n",
        "# Define a custom neural network model\n",
        "class SentimentModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(SentimentModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# One-hot encode the texts using CountVectorizer\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "vectorizer.fit(train_texts)\n",
        "\n",
        "# Define the dataset and data loader for training\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, vectorizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the dataset and data loader for testing\n",
        "val_dataset = SentimentDataset(val_texts, val_labels, vectorizer)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = SentimentModel(input_size=len(vectorizer.vocabulary_), hidden_size1=100, hidden_size2=50, output_size=3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(5):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float().to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f\"[Epoch {epoch+1}, Batch {i+1}] Loss: {running_loss/100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "with torch.no_grad():\n",
        "    for data in val_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs.float().to(device))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set: {(total_correct/total_samples)*100:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3urFNGm67nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "1. Experiment with different activation functions.\n",
        "2. Apply the classifier to different sentiment datasets from hugginface. What did you notice?\n",
        "3. Make the classifiers comparable (similar scores) by tuning hyperparameters.\n",
        "4. Are the features still sub-optimal?"
      ],
      "metadata": {
        "id": "2LpXyx3jQiJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Please try running the code with the \\ of these functions: `['relu', 'leaky_relu', 'tanh', 'sigmoid', 'elu']`, remember we already have `relu` in our code (Hint: `F.relu`)\n",
        "- ReLU (Rectified Linear Unit): The default choice in many neural networks that sets negative values to zero\n",
        "- Leaky ReLU: A variant of ReLU that allows a small gradient for negative values\n",
        "- Tanh: Hyperbolic tangent that outputs values between -1 and 1\n",
        "- Sigmoid: Outputs values between 0 and 1, historically used in neural networks\n",
        "- ELU (Exponential Linear Unit): Similar to ReLU but with smooth negative values (similar activation functions to ELU are used in large language models)\n",
        "\n",
        "You must see something similar to the following behaviours:\n",
        "- ReLU and Leaky ReLU typically converge faster (this may not happen in all cases).\n",
        "- Tanh often provides better accuracy for sentiment analysis tasks but may take a while to converge.\n",
        "- Sigmoid tends to perform worse, especially as you add layers -- try this with your other hyperparameters.\n",
        "- ELU sometimes provides a good balance between fast convergence and good performance\n",
        "\n",
        "3. This depends on your hyperparameter combinations\n",
        "\n",
        "4. We are still using features from our exercise 1 and 2. They are probably not the best. -- we will learn later how we can obtain faster and simpler ways of training deep neural models.\n"
      ],
      "metadata": {
        "id": "7AQ85bBOiCTk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Mw3_q_Ti-2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}