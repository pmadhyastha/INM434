{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/Introduction_to_frontiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjCvsp7RRJij"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Pranava Madhyastha\"\n",
        "__version__ = \"INM434/IN3045 City, University of London, Spring 2025\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Advanced LLM Techniques: RAG, Chain of Thought, and LoRA\n",
        "\n",
        "In this notebook, we will dive into three important techniques that enhance the capabilities and efficiency of Large Language Models (LLMs):\n",
        "\n",
        "1.  **Retrieval Augmented Generation (RAG):** We'll learn how to augment LLMs with external knowledge to overcome their limitations and generate more informed and accurate responses.\n",
        "2.  **Chain of Thought (CoT) Reasoning:** We'll explore how to prompt LLMs to perform step-by-step reasoning, improving their ability to solve complex problems.\n",
        "3.  **Low-Rank Adaptation (LoRA):** We'll also learn how to fine-tune LLMs efficiently using LoRA, a parameter-efficient technique that reduces computational costs and resource requirements."
      ],
      "metadata": {
        "id": "bw5C9xSiRjMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y wandb  # Explicitly uninstall wandb (wandb is a experiment logging tool - have a peak at the tool if you want to)\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # Set environment variable to disable wandb\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "!echo \"WandB disabled forcefully.\"\n",
        "!pip install faiss-gpu-cu12\n",
        "!pip install -U transformers bitsandbytes accelerate datasets # we have seen all these libraries before.\n"
      ],
      "metadata": {
        "id": "68LIBvsFRibF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you begin as usual please make sure, like last time, you are running this notebook in an environment with **GPU acceleration enabled**. In Google Colab, go to \"Runtime\" -> \"Change runtime type\" and select \"A100 GPU\" (or a similar GPU) as the hardware accelerator."
      ],
      "metadata": {
        "id": "w3dFY0sjRv9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "from IPython.display import display\n"
      ],
      "metadata": {
        "id": "PwEEH5-SRtjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Retrieval Augmented Generation (RAG)\n",
        "\n",
        "Large Language Models are powerful language generators, but they have limitations and we have seen this in the lecture:\n",
        "\n",
        "*   **Knowledge \"time and domain\" cut off:** LLMs are trained on data up to a certain point in time and on a large selection of domains. They lack knowledge about recent events or specific domain information not present in their training data.\n",
        "*   **Hallucinations:** LLMs can sometimes generate factually incorrect or nonsensical information, as they are trained to generate text that is statistically likely, not necessarily factually true.\n",
        "\n",
        "RAG is one of the approaches to address these issues by combining the strengths of LLMs with the ability to retrieve information from external knowledge sources.\n",
        "\n",
        "**How RAG Works:**\n",
        "\n",
        "1.  **Retrieval:** Given a user query, RAG first retrieves relevant documents or passages from a knowledge base (e.g., a vector database of documents).\n",
        "2.  **Augmentation:** The retrieved information is then incorporated into the prompt given to the LLM.\n",
        "3.  **Generation:** The LLM uses both its pre-existing knowledge and the retrieved information to generate a more informed and grounded response.\n",
        "\n",
        "Let's implement a simple RAG pipeline! We will use a small dataset of example documents and a pre-trained LLM for text generation.\n",
        "\n",
        "First, let's create a simple **knowledge base**. For this example, we'll use a few sentences about different topics. In a real-world scenario, this could be a large collection of documents, articles, or a specialized database."
      ],
      "metadata": {
        "id": "wd4pcunvSB29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple knowledge base (list of text snippets)\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Eiffel Tower is a famous landmark in Paris.\",\n",
        "    \"Machine learning is a subfield of artificial intelligence.\",\n",
        "    \"Large Language Models are a type of machine learning model.\",\n",
        "    \"RAG stands for Retrieval Augmented Generation.\",\n",
        "    \"LoRA is a parameter-efficient fine-tuning technique.\",\n",
        "    \"Chain of Thought prompting improves reasoning in LLMs.\"\n",
        "]"
      ],
      "metadata": {
        "id": "TneXNuV6SAyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to **index** this knowledge base so we can efficiently search for relevant information. We will use **FAISS** (Facebook AI Similarity Search) to create a vector database.\n",
        "\n",
        "First, we need to convert our text snippets into **vector embeddings**. Embeddings are numerical representations of text that capture their semantic meaning. We'll use a pre-trained sentence transformer model from Hugging Face for this."
      ],
      "metadata": {
        "id": "U-NI8NHJSHLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Load a sentence transformer model and tokenizer\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # A fast and efficient sentence embedding model\n",
        "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
        "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
        "\n",
        "# Function to get embeddings for text\n",
        "def get_embeddings(texts):\n",
        "    encoded_input = embedding_tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = embedding_model(**encoded_input)\n",
        "    # Perform pooling. In this case, Mean Pooling - Take attention mask into account for correct averaging\n",
        "    input_mask_expanded = encoded_input['attention_mask'].unsqueeze(-1).expand(model_output.last_hidden_state.size()).float()\n",
        "    sum_embeddings = torch.sum(model_output.last_hidden_state * input_mask_expanded, 1)\n",
        "    sum_mask = torch.sum(input_mask_expanded, 1)\n",
        "    mean_pooled = sum_embeddings / sum_mask\n",
        "    return mean_pooled.cpu().numpy()\n",
        "\n",
        "# Get embeddings for our knowledge base\n",
        "knowledge_embeddings = get_embeddings(knowledge_base)\n",
        "print(\"Embeddings shape:\", knowledge_embeddings.shape)"
      ],
      "metadata": {
        "id": "_OFmf5wUSG1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have generated embeddings for each text snippet in our knowledge base. Now, let's build a **FAISS index** to store these embeddings and enable efficient similarity search. We will use a simple IndexFlatL2 index for this demonstration, which is suitable for smaller datasets. For larger datasets, you might consider more advanced indexing techniques offered by FAISS."
      ],
      "metadata": {
        "id": "x2hTTNS5SL_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = knowledge_embeddings.shape[1] # Dimension of embeddings\n",
        "index = faiss.IndexFlatL2(dimension) # L2 or Euclidean distance for similarity -- TODO -- change it to other distance functions!!\n",
        "index.add(knowledge_embeddings)"
      ],
      "metadata": {
        "id": "LHb3xJV0SLan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have our knowledge base indexed! Let's create a **retrieval function** that takes a user query, generates its embedding, and retrieves the most similar documents from the knowledge base using the FAISS index."
      ],
      "metadata": {
        "id": "FwDLR1XWTyVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_documents(query, top_k=2): # Retrieve top 2 most relevant documents by default\n",
        "    query_embedding = get_embeddings([query])\n",
        "    D, I = index.search(query_embedding, top_k) # Search top_k most similar embeddings\n",
        "    retrieved_documents = [knowledge_base[i] for i in I[0]] # Get corresponding documents\n",
        "    return retrieved_documents"
      ],
      "metadata": {
        "id": "-Hy_VWLAT012"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try an example to retrieve."
      ],
      "metadata": {
        "id": "7Zk44GWiT5d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_rag = \"What is RAG?\"\n",
        "relevant_docs = retrieve_relevant_documents(query_rag)\n",
        "print(f\"Query: '{query_rag}'\")\n",
        "print(\"Retrieved documents:\")\n",
        "for doc in relevant_docs:\n",
        "    print(f\"- {doc}\")"
      ],
      "metadata": {
        "id": "Nms37p5eT3jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Our retrieval function is working. Now, let's integrate this with an **LLM for text generation**. We'll use the `distilgpt2` model again, similar to the previous notebook.\n",
        "\n",
        "We will construct a **prompt** that includes both the user query and the retrieved relevant documents. This combined prompt will be given to the LLM to generate a RAG-enhanced response."
      ],
      "metadata": {
        "id": "Qe74TYWxUD-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llm_model_name = \"distilgpt2\"\n",
        "llm_generator = pipeline('text-generation', model=llm_model_name)\n",
        "\n",
        "def generate_rag_response(query):\n",
        "    relevant_documents = retrieve_relevant_documents(query)\n",
        "    context = \"\\n\".join(relevant_documents) # Combine retrieved documents into context\n",
        "    prompt = f\"Context information: {context}\\n\\nUser Query: {query}\\n\\nAnswer:\" # Construct RAG prompt\n",
        "    rag_output = llm_generator(prompt, max_length=150, num_return_sequences=1, pad_token_id=llm_generator.tokenizer.eos_token_id) # Generate response\n",
        "    return rag_output[0]['generated_text']\n",
        "\n",
        "# Example RAG response generation\n",
        "query_example_rag = \"Tell me about Paris.\"\n",
        "rag_response = generate_rag_response(query_example_rag)\n",
        "print(f\"RAG Query: '{query_example_rag}'\")\n",
        "print(f\"RAG Response: '{rag_response}'\")\n"
      ],
      "metadata": {
        "id": "NtsxMEEPUBa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examine the RAG Response:**\n",
        "\n",
        "*   Does the response incorporate information from the retrieved documents?\n",
        "*   Is the response more informative and relevant compared to just asking the LLM directly without RAG?\n",
        "\n",
        "Let's create an **interactive interface** using `ipywidgets` so you can experiment with different queries and see RAG in action!"
      ],
      "metadata": {
        "id": "qpEx0z2CUI9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_interface(query_text):\n",
        "    rag_response_text = generate_rag_response(query_text)\n",
        "    print(f\"Query: '{query_text}'\\n\")\n",
        "    print(f\"RAG Response: '{rag_response_text}'\")\n",
        "\n",
        "query_input_widget_rag = widgets.Textarea(\n",
        "    value='Ask me something about the topics in the knowledge base...',\n",
        "    placeholder='Type your query here',\n",
        "    description='Query:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%', height='100px')\n",
        ")\n",
        "\n",
        "interactive_rag = interactive(rag_interface, query_text=query_input_widget_rag)\n",
        "display(interactive_rag)\n"
      ],
      "metadata": {
        "id": "lLwBMFcAUHgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try it out!**\n",
        "\n",
        "*   Enter different queries related to the topics in our `knowledge_base` (Paris, Eiffel Tower, Machine Learning, RAG, LoRA, Chain of Thought).\n",
        "*   Observe how RAG uses the retrieved context to generate answers.\n",
        "*   Try queries outside of the knowledge base topics. How does RAG behave in those cases?\n",
        "*   Can you try a different model (perhaps a larger model)?\n",
        "\n",
        "\n",
        "\n",
        "**TODOs for RAG Exploration:**\n",
        "\n",
        "1.  **Experiment with different knowledge bases:**\n",
        "    *   Replace the simple `knowledge_base` with a larger text file or a dataset from Hugging Face Datasets ([https://huggingface.co/datasets](https://huggingface.co/datasets)).\n",
        "    *   How does the performance of RAG change with a larger and more diverse knowledge base?\n",
        "2.  **Explore different embedding models:**\n",
        "    *   Try other sentence transformer models from Hugging Face ([https://huggingface.co/sentence-transformers](https://huggingface.co/sentence-transformers)). Some models are more accurate but slower, while others are faster but potentially less accurate.\n",
        "    *   How does the choice of embedding model affect retrieval quality and RAG response?\n",
        "3.  **Investigate different retrieval strategies:**\n",
        "    *   Instead of simple similarity search with FAISS, research and implement more advanced retrieval techniques like:\n",
        "        *   Keyword-based retrieval (e.g., using TF-IDF or BM25).\n",
        "        *   Hybrid retrieval (combining keyword and semantic search).\n",
        "    *   How do these different strategies impact the relevance of retrieved documents and the overall RAG performance?\n",
        "4.  **Evaluate RAG qualitatively and quantitatively:**\n",
        "    *   **Qualitative Evaluation:** Manually examine the RAG responses for different queries. Are they factually correct? Do they answer the query effectively? Are they coherent and well-written?\n",
        "    *   **Quantitative Evaluation (more advanced):** If you have access to ground truth answers for your queries, you can use metrics like:\n",
        "        *   **Recall@k:**  Does the correct answer appear in the top-k retrieved documents?\n",
        "        *   **Answer Relevance:**  How relevant is the generated answer to the query? (Requires more sophisticated evaluation methods)."
      ],
      "metadata": {
        "id": "sSJEq59OUcq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Chain of Thought (CoT) Reasoning\n",
        "\n",
        "Large Language Models can sometimes struggle with complex reasoning tasks that require multiple steps or logical inference. **Chain of Thought (CoT) prompting** is a technique that encourages LLMs to break down complex problems into smaller, more manageable steps, mimicking human-like step-by-step reasoning.\n",
        "\n",
        "**How CoT Works:**\n",
        "\n",
        "Instead of directly asking the LLM for the final answer, we prompt it to **show its reasoning process**.  This is typically done by adding phrases like \"Let's think step by step\" or \"Reasoning process:\" to the prompt.  By explicitly prompting for the reasoning chain, we guide the LLM to generate intermediate steps before arriving at the final answer.\n",
        "\n",
        "Let's see an example. Consider a simple arithmetic problem:\n",
        "\n",
        "**Problem:** \"If I have 3 apples and I give 2 to my friend, how many apples do I have left?\"\n",
        "\n",
        "Let's try prompting the LLM in two ways:\n",
        "\n",
        "1.  **Direct Prompting (without CoT):** Just ask the question directly.\n",
        "2.  **CoT Prompting (with CoT):**  Prompt the LLM to think step by step.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "SrCoyPFQUuBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_llm_response(prompt_cot, model_name_cot=\"distilgpt2\"): # Function to generate text from LLM\n",
        "    generator_cot = pipeline('text-generation', model=model_name_cot)\n",
        "    output_cot = generator_cot(prompt_cot, max_length=100, num_return_sequences=1, pad_token_id=generator_cot.tokenizer.eos_token_id)\n",
        "    return output_cot[0]['generated_text']\n",
        "\n",
        "# Example Problem\n",
        "arithmetic_problem = \"If I have 3 apples and I give 2 to my friend, how many apples do I have left?\"\n",
        "\n",
        "# 1. Direct Prompting\n",
        "direct_prompt = f\"Question: {arithmetic_problem}\\nAnswer:\"\n",
        "direct_response = generate_llm_response(direct_prompt)\n",
        "print(\"Direct Prompt Response:\\n\", direct_response)\n",
        "\n",
        "# 2. CoT Prompting\n",
        "cot_prompt = f\"Question: {arithmetic_problem}\\nLet's think step by step:\"\n",
        "cot_response = generate_llm_response(cot_prompt)\n",
        "print(\"\\nChain of Thought Response:\\n\", cot_response)\n"
      ],
      "metadata": {
        "id": "Qa4uBhqpUVTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examine the Responses:**\n",
        "\n",
        "*   Compare the direct response and the CoT response.\n",
        "*   Does the CoT response show any steps of reasoning? Even if it doesn't perfectly solve the arithmetic, does it attempt to break down the problem?\n",
        "*   Note: `distilgpt2` is a smaller model and might not be ideal for complex reasoning, but we can still observe the effect of CoT prompting.\n",
        "\n",
        "Let's try a slightly more complex problem and see if CoT makes a difference."
      ],
      "metadata": {
        "id": "Z1u7usIfU9Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logic_problem = \"A train leaves Chicago at 6am traveling at 60mph towards Denver. Another train leaves Denver at 8am traveling at 70mph towards Chicago. When will they meet?\"\n",
        "\n",
        "# Direct Prompting for logic problem\n",
        "direct_prompt_logic = f\"Question: {logic_problem}\\nAnswer:\"\n",
        "direct_response_logic = generate_llm_response(direct_prompt_logic)\n",
        "print(\"\\nDirect Prompt Response (Logic Problem):\\n\", direct_response_logic)\n",
        "\n",
        "# CoT Prompting for logic problem\n",
        "cot_prompt_logic = f\"Question: {logic_problem}\\nLet's think step by step:\"\n",
        "cot_response_logic = generate_llm_response(cot_prompt_logic)\n",
        "print(\"\\nChain of Thought Response (Logic Problem):\\n\", cot_response_logic)"
      ],
      "metadata": {
        "id": "cZIiBccUU2Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examine the Responses for the Logic Problem:**\n",
        "\n",
        "*   Is there a noticeable difference between the direct response and the CoT response for this more complex problem?\n",
        "*   Does CoT prompting encourage the LLM to at least attempt a step-by-step approach, even if it doesn't solve the problem correctly?\n",
        "\n",
        "**Important Note:** The effectiveness of CoT prompting can depend on:\n",
        "\n",
        "*   **Model Size and Capabilities:** Larger and more capable LLMs are generally better at CoT reasoning. Smaller models like `distilgpt2` might show limited CoT abilities.\n",
        "*   **Prompt Engineering:** The way you phrase your CoT prompt can significantly impact the results. Experiment with different CoT prompting phrases and formats.\n",
        "*   **Task Complexity:** CoT is most beneficial for tasks that genuinely require multi-step reasoning. For very simple tasks, direct prompting might be sufficient.\n",
        "\n",
        "Let's create an **interactive interface** to experiment with CoT prompting for different questions!"
      ],
      "metadata": {
        "id": "kOghXfwGVDEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cot_interface(question_text, use_cot):\n",
        "    if use_cot:\n",
        "        cot_prompt_interactive = f\"Question: {question_text}\\nLet's think step by step:\"\n",
        "        response_text = generate_llm_response(cot_prompt_interactive)\n",
        "        print(f\"Prompt (CoT): '{cot_prompt_interactive}'\\n\")\n",
        "        print(f\"CoT Response: '{response_text}'\")\n",
        "    else:\n",
        "        direct_prompt_interactive = f\"Question: {question_text}\\nAnswer:\"\n",
        "        response_text = generate_llm_response(direct_prompt_interactive)\n",
        "        print(f\"Prompt (Direct): '{direct_prompt_interactive}'\\n\")\n",
        "        print(f\"Direct Response: '{response_text}'\")\n",
        "\n",
        "question_input_widget_cot = widgets.Textarea(\n",
        "    value='Ask me a question that might require some reasoning...',\n",
        "    placeholder='Type your question here',\n",
        "    description='Question:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%', height='100px')\n",
        ")\n",
        "cot_checkbox_widget = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Use Chain of Thought (CoT)',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "interactive_cot = interactive(cot_interface, question_text=question_input_widget_cot, use_cot=cot_checkbox_widget)\n",
        "display(widgets.VBox([question_input_widget_cot, cot_checkbox_widget, interactive_cot]))"
      ],
      "metadata": {
        "id": "kKCLtxSBVBkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try it out!**\n",
        "\n",
        "*   Enter different questions that might require reasoning (arithmetic, logic, common sense reasoning, etc.).\n",
        "*   Toggle the \"Use Chain of Thought (CoT)\" checkbox to compare responses with and without CoT prompting.\n",
        "*   Observe if CoT prompting makes a difference in the quality of the responses, especially for more complex questions.\n",
        "\n",
        "**TODOs for CoT Exploration:**\n",
        "\n",
        "1.  **Experiment with different CoT prompt formats:**\n",
        "    *   Try different phrases to initiate CoT prompting, such as:\n",
        "        *   \"Let's work this out step by step.\"\n",
        "        *   \"Reasoning:\"\n",
        "        *   \"First, I will...\" \"Then, I will...\" \"Finally, I will...\" (more explicit step-by-step prompting)\n",
        "    *   How do different prompt formats influence the LLM's reasoning process and output?\n",
        "2.  **Apply CoT to different types of reasoning tasks:**\n",
        "    *   Explore CoT prompting for tasks beyond arithmetic and logic, such as:\n",
        "        *   Common sense reasoning questions.\n",
        "        *   Textual entailment or contradiction detection.\n",
        "        *   Simple planning or decision-making problems.\n",
        "    *   For which types of tasks is CoT most effective?\n",
        "3.  **Investigate techniques to improve CoT reliability and accuracy:**\n",
        "    *   Research more advanced CoT techniques like:\n",
        "        *   **Self-Consistency in CoT:** Generate multiple CoT reasoning paths and select the most consistent answer.\n",
        "        *   **Least-to-Most Prompting:** Break down a complex problem into a sequence of simpler subproblems and solve them in order, building up to the final solution.\n",
        "    *   How can these techniques further enhance the power of CoT reasoning?\n",
        "4.  **Evaluate CoT performance:**\n",
        "    *   Design experiments to quantitatively evaluate the impact of CoT prompting on reasoning accuracy for specific tasks.\n",
        "    *   Compare the performance of LLMs with and without CoT prompting using appropriate evaluation metrics."
      ],
      "metadata": {
        "id": "bZI0efbFVQgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Low-Rank Adaptation (LoRA) for Efficient Fine-tuning\n",
        "\n",
        "Fine-tuning Large Language Models can be computationally expensive and resource-intensive, especially for very large models. **Low-Rank Adaptation (LoRA)** is a parameter-efficient fine-tuning technique that significantly reduces the number of trainable parameters, making fine-tuning more accessible and efficient.\n",
        "\n",
        "**How LoRA Works:**\n",
        "\n",
        "LoRA freezes the pre-trained weights of the LLM and introduces a small number of **new trainable parameters** in the form of low-rank matrices. During fine-tuning, only these low-rank matrices are updated, while the original LLM weights remain unchanged.\n",
        "\n",
        "**Benefits of LoRA:**\n",
        "\n",
        "*   **Parameter Efficiency:**  Reduces the number of trainable parameters by orders of magnitude (e.g., from billions to millions or even less).\n",
        "*   **Reduced Computational Cost:**  Faster training and lower memory requirements due to fewer trainable parameters.\n",
        "*   **Preserves Pre-trained Knowledge:**  Freezing the original weights helps retain the general knowledge and capabilities of the pre-trained LLM.\n",
        "*   **Modular and Portable:**  LoRA adapters (the low-rank matrices) are small and can be easily swapped or combined for different tasks without modifying the base LLM.\n",
        "\n",
        "Let's demonstrate LoRA fine-tuning using the `peft` (Parameter-Efficient Fine-Tuning) library from Hugging Face. We will fine-tune `distilgpt2` for a simple text classification task (e.g., sentiment analysis, although we'll simplify it for demonstration).\n",
        "\n",
        "First, let's load a dataset for text classification. We'll use a very small, synthetic dataset for quick demonstration purposes. In a real scenario, you would use a larger and more representative dataset for your target task.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m4B_IXbIVVM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic dataset for demonstration (replace with a real dataset for meaningful fine-tuning)\n",
        "lora_dataset_dict = {\n",
        "    \"text\": [\n",
        "        \"This is a great movie!\",\n",
        "        \"I really enjoyed this book.\",\n",
        "        \"The food was terrible.\",\n",
        "        \"I hated this product.\",\n",
        "        \"Excellent service and quality.\",\n",
        "        \"Very disappointing experience.\"\n",
        "    ],\n",
        "    \"label\": [1, 1, 0, 0, 1, 0] # 1 for positive, 0 for negative (arbitrary labels for demonstration)\n",
        "}\n",
        "\n",
        "from datasets import Dataset\n",
        "lora_dataset = Dataset.from_dict(lora_dataset_dict)\n",
        "print(lora_dataset) # Examine the synthetic dataset\n",
        "\n",
        "\"\"\"Now, let's tokenize this dataset using the tokenizer for `distilgpt2`. This is the same tokenization process we used in the previous notebook for sentiment analysis fine-tuning.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "lora_model_name = \"distilgpt2\"\n",
        "lora_tokenizer = AutoTokenizer.from_pretrained(lora_model_name)\n",
        "lora_tokenizer.pad_token = lora_tokenizer.eos_token # Important: Set pad token\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_lora_function(examples):\n",
        "    return lora_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_lora_dataset = lora_dataset.map(tokenize_lora_function, batched=True)\n",
        "print(tokenized_lora_dataset) # Inspect the tokenized dataset"
      ],
      "metadata": {
        "id": "9fPS8c6gVI-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Next, we need to load the `distilgpt2` model for **sequence classification** (since we're doing text classification). We use `AutoModelForSequenceClassification` as before.\n",
        "\n",
        "Now, the crucial step: we will apply **LoRA** to this model using the `get_peft_model` function from the `peft` library. We need to configure `LoraConfig` to specify the LoRA parameters, such as the rank `r` and the alpha parameter."
      ],
      "metadata": {
        "id": "3VwoVXYmW_nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Load model for sequence classification\n",
        "lora_base_model = AutoModelForSequenceClassification.from_pretrained(lora_model_name, num_labels=2) # 2 labels for our example\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # Rank of the low-rank matrices\n",
        "    lora_alpha=32, # Scaling factor for LoRA layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\" # Task type is sequence classification\n",
        ")\n",
        "\n",
        "# Apply LoRA to the base model\n",
        "lora_model = get_peft_model(lora_base_model, lora_config)\n",
        "lora_model.config.pad_token_id = lora_model.config.eos_token_id # Set pad token for LoRA model\n",
        "lora_model.print_trainable_parameters() # Print the number of trainable parameters"
      ],
      "metadata": {
        "id": "0soqdwziXFjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examine the Trainable Parameters:**\n",
        "\n",
        "*   Run the `lora_model.print_trainable_parameters()` line.\n",
        "*   Compare the number of trainable parameters with LoRA to the total number of parameters in `distilgpt2` (which is around 82 million).\n",
        "*   You should see a significantly smaller number of trainable parameters with LoRA, demonstrating its parameter efficiency.\n",
        "\n",
        "Now, let's set up the **training arguments** and the **Trainer** from Hugging Face `transformers`, just like we did for fine-tuning in the previous notebook. The training process for LoRA is very similar to regular fine-tuning."
      ],
      "metadata": {
        "id": "ozSPghMbV0v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "lora_training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-distilgpt2-classification\",\n",
        "    learning_rate=2e-4, # Higher learning rate often used for LoRA\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3, # Small number of epochs for demonstration\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    push_to_hub=False,\n",
        "    report_to=None, # Disable WandB\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "lora_trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=lora_training_args,\n",
        "    train_dataset=tokenized_lora_dataset,\n",
        "    eval_dataset=tokenized_lora_dataset, # Using train dataset as eval for simplicity in demonstration\n",
        "    tokenizer=lora_tokenizer,\n",
        "    # metrics can be added if needed\n",
        ")\n",
        "\n",
        "lora_trainer.train() # Start LoRA fine-tuning\n"
      ],
      "metadata": {
        "id": "8ncMtFaSVtxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `lora_trainer.train()` command starts the LoRA fine-tuning process. This will be much faster and less memory-intensive than full fine-tuning of `distilgpt2`.\n",
        "\n",
        "After training, let's evaluate the LoRA-fine-tuned model and see how it performs on our synthetic dataset."
      ],
      "metadata": {
        "id": "POCUiuShV_Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_lora_sentiment(sentence):\n",
        "    inputs = lora_tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(lora_model.device)\n",
        "    outputs = lora_model(**inputs)\n",
        "    predictions = outputs.logits.argmax(dim=-1)\n",
        "    sentiment_lora = \"positive\" if predictions.item() == 1 else \"negative\" # Assuming label 1 is positive, 0 is negative\n",
        "    return f\"Sentiment (LoRA): {sentiment_lora}\"\n",
        "\n",
        "lora_sentences_to_test = [\n",
        "    \"This movie was fantastic!\",\n",
        "    \"I am really disappointed with this.\",\n",
        "    \"It was okay, nothing special.\"\n",
        "]\n",
        "\n",
        "print(\"\\nSentiment Predictions from LoRA Fine-tuned GPT-2:\")\n",
        "for sentence in lora_sentences_to_test:\n",
        "    print(f\"- Sentence: '{sentence}' - {predict_lora_sentiment(sentence)}\")"
      ],
      "metadata": {
        "id": "2A-h-saeWBo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observe the LoRA Predictions:**\n",
        "\n",
        "*   Do the sentiment predictions from the LoRA-fine-tuned model seem reasonable for the example sentences?\n",
        "*   Remember that we fine-tuned on a very small and synthetic dataset, so the model's performance might be limited.\n",
        "\n",
        "**TODOs for LoRA Exploration:**\n",
        "\n",
        "1.  **Experiment with different LoRA configurations:**\n",
        "    *   Change the LoRA rank `r` (e.g., try r=4, r=16, r=32). Higher rank generally allows for more expressiveness but also increases the number of trainable parameters.\n",
        "    *   Adjust the `lora_alpha` parameter. This scales the LoRA matrices.\n",
        "    *   How do these configuration changes affect the fine-tuning process, model performance, and the number of trainable parameters?\n",
        "2.  **Apply LoRA to different tasks and datasets:**\n",
        "    *   Fine-tune `distilgpt2` (or other models) using LoRA for different NLP tasks, such as:\n",
        "        *   Sentiment analysis on a real dataset (e.g., IMDB reviews, as in the previous notebook).\n",
        "        *   Text summarization.\n",
        "        *   Question answering.\n",
        "    *   Explore datasets on Hugging Face Datasets ([https://huggingface.co/datasets](https://huggingface.co/datasets)) for various tasks.\n",
        "3.  **Compare LoRA with full fine-tuning (conceptually):**\n",
        "    *   Think about the advantages and disadvantages of LoRA compared to full fine-tuning, especially in terms of:\n",
        "        *   Computational cost and resource requirements.\n",
        "        *   Fine-tuning speed.\n",
        "        *   Number of trainable parameters.\n",
        "        *   Potential performance differences (LoRA might sometimes be slightly less performant than full fine-tuning but is much more efficient).\n",
        "4.  **Investigate other parameter-efficient fine-tuning methods (conceptually):**\n",
        "    *   have a look at other parameter-efficient fine-tuning techniques besides LoRA, such as:\n",
        "        *   Adapter layers.\n",
        "        *   Prefix tuning.\n",
        "        *   Prompt tuning.\n",
        "    *   How do these methods compare to LoRA in terms of efficiency, implementation complexity, and performance?"
      ],
      "metadata": {
        "id": "whAS0hOhXVmS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFGV_YWXXXKp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}