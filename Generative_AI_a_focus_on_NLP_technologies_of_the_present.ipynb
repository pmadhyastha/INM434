{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoAMaV5413oAOPO0fYzD2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/Generative_AI_a_focus_on_NLP_technologies_of_the_present.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers datasets accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "28gFbWDuM3HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft trl"
      ],
      "metadata": {
        "id": "Tnyk2_YcNSFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will now see the ease of using pre-trained models for text generation tasks with just a few lines of code using the Hugging Face Transformers library."
      ],
      "metadata": {
        "id": "WMf5pZ2NL1Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to swtich to GPU runtime."
      ],
      "metadata": {
        "id": "x6wTIcJIMcY8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXBzu009LuN-"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "generator = pipeline('text-generation', model=model_name)\n",
        "\n",
        "prompt = \"I'd like to write a poem about the beauty of nature.\"\n",
        "result = generator(prompt, max_length=50, num_return_sequences=3)\n",
        "\n",
        "for output in result:\n",
        "    print(output['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What does the model generate?"
      ],
      "metadata": {
        "id": "Ned8uVcEMmwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  In the following code, we will finetune GPT2 for the task of classification for sentiment analysis. We will begin with a standard sentiment related dataset - the imdb reviews dataset."
      ],
      "metadata": {
        "id": "GUtmylcMTXIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a sentiment analysis dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "uSNxNLfTORhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  In the above codebase, we load the IMDB sentiment analysis dataset.\n",
        "\n",
        "###  Then we use the GPT-2 tokenizer, and apply the tokenizer to tokenize the text data in the dataset. The resulting tokenized_datasets variable will contain the tokenized text data.\n",
        "\n",
        "###  Now we will set up a training pipeline for the task, using the pre-trained GPT-2 language model and the Hugging Face Transformers library.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EyJUmgeCTtwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "from transformers import GPT2ForSequenceClassification\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./outputs\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_I5Vw3_4OSbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Does this crash? Can you reason why this might be? Can you change the model to `distillGPT2` as we had done before?\n",
        "\n",
        "consider using: `from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer`"
      ],
      "metadata": {
        "id": "9ZHHXRYPUmrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the code for evaluating the model that we have trained!\n",
        "\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "XUaMduc6Sn74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Are you able to evaluate the model? What do you see?  \n",
        "\n",
        "###  In the following codebase, we will perform inference with the model."
      ],
      "metadata": {
        "id": "HAgMhGf-VDLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    prediction = outputs.logits.argmax(-1).item()\n",
        "    return \"positive\" if prediction == 1 else \"negative\"\n",
        "\n",
        "sentence = \"Today's class was interesting but a tad complex!\"\n",
        "print(predict(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo--UMolUq8K",
        "outputId": "2dc1c9f4-2d80-461a-84df-f640f01c0148"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9_J2MqQeVsF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}