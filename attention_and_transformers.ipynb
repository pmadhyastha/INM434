{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6SOud6jij0xSTo00FAoSK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/INM434/blob/main/attention_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3vWwcjyUfZz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [\n",
        "    (\"The cat is on the mat\", [\"DET\", \"NOUN\", \"VERB\", \"ADP\", \"DET\", \"NOUN\"]),\n",
        "    (\"The dog ate my homework\", [\"DET\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\"]),\n",
        "    (\"I love to eat pizza\", [\"PRON\", \"VERB\", \"TO\", \"VERB\", \"NOUN\"]),\n",
        "    (\"She runs every morning\", [\"PRON\", \"VERB\", \"ADV\", \"NOUN\"]),\n",
        "    (\"He drinks coffee and tea\", [\"PRON\", \"VERB\", \"NOUN\", \"CONJ\", \"NOUN\"]),\n",
        "]\n",
        "\n",
        "test_data = [\n",
        "    (\"The bird is flying\", [\"DET\", \"NOUN\", \"VERB\", \"VERB\"]),\n",
        "    (\"They are playing soccer\", [\"PRON\", \"VERB\", \"VERB\", \"NOUN\"]),\n",
        "]\n"
      ],
      "metadata": {
        "id": "v961yvY7UnUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class POSDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence, tags = self.data[idx]\n",
        "        sentence = sentence.lower().split()\n",
        "        return sentence, tags\n"
      ],
      "metadata": {
        "id": "X1M0QHejUoul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(data):\n",
        "    vocab = set()\n",
        "    for sentence, _ in data:\n",
        "        for word in sentence.lower().split():\n",
        "            vocab.add(word)\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "aMB9u-8qUp9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class POSModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedded = self.embedding(sentence)\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        attention_weights = torch.softmax(torch.bmm(outputs, hidden[0].unsqueeze(2)), dim=1)\n",
        "        attention = torch.bmm(outputs.transpose(1, 2), attention_weights).squeeze(2)\n",
        "\n",
        "        output = self.fc(attention)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "YDafljuyUriM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(build_vocab(training_data))\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 100\n",
        "OUTPUT_DIM = len(set(tag for _, tags in training_data for tag in tags))\n",
        "\n",
        "model = POSModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n"
      ],
      "metadata": {
        "id": "cMGNLRMGUswn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n"
      ],
      "metadata": {
        "id": "XyprlRBkUuAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, loss_fn, optimizer, num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    train_dataset = POSDataset(train_data)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=2)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, (sentences, tags) in enumerate(train_dataloader):\n",
        "            # Convert sentences and tags to PyTorch tensors\n",
        "            sentences = torch.LongTensor([[vocab[word] for word in sentence] for sentence in sentences])\n",
        "            tags = torch.LongTensor([[tag2idx[tag] for tag in sentence] for sentence in tags])\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(sentences)\n",
        "\n",
        "            # Flatten the outputs and tags\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = loss_fn(outputs, tags)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Print the average loss for the epoch\n",
        "        print(f\"Epoch {epoch + 1} loss: {total_loss / len(train_dataset)}\")\n"
      ],
      "metadata": {
        "id": "bIYa8D5OUviz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data):\n",
        "    model.eval()\n",
        "\n",
        "    test_dataset = POSDataset(test_data)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        total_count = 0\n",
        "        for sentences, tags in test_dataloader:\n",
        "            # Convert sentences to PyTorch tensors\n",
        "            sentences = torch.LongTensor([[vocab[word] for word in sentence] for sentence in sentences])\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(sentences)\n",
        "\n",
        "            # Convert the outputs to predicted tags\n",
        "            _, predicted_tags = torch.max(outputs, dim=-1)\n",
        "            predicted_tags = predicted_tags.tolist()\n",
        "\n",
        "            # Compute the number of correct predictions\n",
        "            for i, sentence in enumerate(predicted_tags):\n",
        "                for j, tag in enumerate(sentence):\n",
        "                    if tag == tag2idx[tags[i][j]]:\n",
        "                        total_correct += 1\n",
        "                    total_count += 1\n",
        "\n",
        "        # Print the accuracy\n",
        "        accuracy = total_correct / total_count\n",
        "        print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "NOCzYIWoUyOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the vocabulary and tag dictionaries\n",
        "vocab = {word: i for i, word in enumerate(build_vocab(training_data))}\n",
        "tag2idx = {tag: i for i, tag in enumerate(set(tag for _, tags in training_data for tag in tags))}\n",
        "idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
        "\n",
        "# Train the model\n",
        "train(model, train_dataloader, loss_fn, optimizer, num_epochs=10)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "evaluate(model, test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "uN__VF9-Uzl8",
        "outputId": "3058c103-e61b-47ff-d664-0ada20f7f811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-67c987cd7838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Evaluate the model on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-90c91f75c0d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, loss_fn, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Convert sentences and tags to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-41eeee0e1201>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim)\n",
        "        self.attention = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        outputs = []\n",
        "        for input in input_seq:\n",
        "            hidden_state = self.rnn(input)\n",
        "            attention_weights = self.attention(hidden_state)\n",
        "            output = torch.matmul(attention_weights, input)\n",
        "            outputs.append(output)\n",
        "        return outputs\n",
        "\n",
        "def main():\n",
        "    # Create the training data\n",
        "    training_data = []\n",
        "    for i in range(100):\n",
        "        input_seq = [torch.randint(10, 100) for _ in range(10)]\n",
        "        output_seq = [torch.randint(10, 10) for _ in range(10)]\n",
        "        training_data.append((input_seq, output_seq))\n",
        "\n",
        "    # Create the model\n",
        "    model = RNN(10, 10, 10)\n",
        "\n",
        "    # Train the model\n",
        "    model.train(training_data, epochs=100)\n",
        "\n",
        "    # Test the model\n",
        "    test_data = []\n",
        "    for i in range(10):\n",
        "        input_seq = [torch.randint(10, 100) for _ in range(10)]\n",
        "        output_seq = [torch.randint(10, 10) for _ in range(10)]\n",
        "        test_data.append((input_seq, output_seq))\n",
        "\n",
        "    # Get the predictions\n",
        "    predictions = model(test_data)\n",
        "\n",
        "    # Compute the accuracy\n",
        "    accuracy = torch.sum(torch.argmax(predictions, axis=1) == output_seq) / 10\n",
        "\n",
        "    # Print the accuracy\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "P57_YLbeU1IE",
        "outputId": "3d2f9fb8-31c7-447d-e2e4-a2b8d57d1a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-91c92f32ea5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-91c92f32ea5c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moutput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-91c92f32ea5c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moutput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: randint(): argument 'size' (position 2) must be tuple of ints, not int"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Training corpus\n",
        "train_data = [[(\"The\", \"DT\"), (\"dog\", \"NN\"), (\"ran\", \"VBD\"), (\"across\", \"IN\"), (\"the\", \"DT\"), (\"field\", \"NN\")],\n",
        "              [(\"The\", \"DT\"), (\"cat\", \"NN\"), (\"jumped\", \"VBD\"), (\"over\", \"IN\"), (\"the\", \"DT\"), (\"moon\", \"NN\")]]\n",
        "\n",
        "# Test corpus\n",
        "test_data = [(\"The\", \"DT\"), (\"cow\", \"NN\"), (\"jumped\", \"VBD\"), (\"over\", \"IN\"), (\"the\", \"DT\"), (\"moon\", \"NN\")]\n",
        "\n",
        "# Create dictionary for mapping words to indices\n",
        "word2idx = {}\n",
        "for sentence in train_data:\n",
        "    for word, _ in sentence:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word] = len(word2idx)\n",
        "\n",
        "# Create dictionary for mapping tags to indices\n",
        "tag2idx = {\"DT\": 0, \"NN\": 1, \"VBD\": 2, \"IN\": 3}\n",
        "\n",
        "# Define RNN with attention-based modeling\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.attn = nn.Linear(hidden_size * 2, 1)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        attn_weights = torch.softmax(self.attn(torch.cat((output[0], hidden[0].squeeze(0)), 1)), dim=1)\n",
        "        output = output.transpose(0, 2).transpose(1, 2)  # Transpose to (1, hidden_size, 1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), output.squeeze(0).unsqueeze(0))\n",
        "        output = self.out(attn_applied[0])\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size),\n",
        "                torch.zeros(1, 1, self.hidden_size))\n",
        "\n",
        "\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = RNN(len(word2idx), 128, len(tag2idx))\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    for sentence in train_data:\n",
        "        model.zero_grad()\n",
        "        hidden = model.init_hidden()\n",
        "        sentence_tensor = torch.tensor([word2idx[word] for word, _ in sentence], dtype=torch.long)\n",
        "        targets = torch.tensor([tag2idx[tag] for _, tag in sentence], dtype=torch.long)\n",
        "        loss = 0\n",
        "        for i in range(len(sentence)):\n",
        "            output, hidden, attn_weights = model(sentence_tensor[i], hidden)\n",
        "            loss += nn.functional.cross_entropy(output, targets[i].unsqueeze(0))\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "0dVKmsSsVIaV",
        "outputId": "e7aaf9a4-11d0-4d08-9dc3-bf8b2d0819f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-36a1a164f9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-36a1a164f9f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Transpose to (1, hidden_size, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mattn_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_applied\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: batch2 must be a 3D tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def randint(low, high):\n",
        "    return torch.randint(low, high+1)\n"
      ],
      "metadata": {
        "id": "G0AmjE8pZVEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define LSTM model\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "# Define training data and word-to-index and tag-to-index dictionaries\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {word: i for i, word in enumerate(set(word for sent, _ in training_data for word in sent))}\n",
        "tag_to_ix = {tag: i for i, tag in enumerate(set(tag for _, tags in training_data for tag in tags))}\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "print(tag_to_ix)\n",
        "print(ix_to_tag)\n",
        "# Define hyperparameters\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(300):\n",
        "    for sentence, tags in training_data:\n",
        "        # Clear out gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Prepare inputs for the network\n",
        "        sentence_in = torch.tensor([word_to_ix[word] for word in sentence], dtype=torch.long)\n",
        "        targets = torch.tensor([tag_to_ix[tag] for tag in tags], dtype=torch.long)\n",
        "\n",
        "        # Forward pass\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Compute loss, gradients, and update parameters\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test the model on a sample sentence\n",
        "with torch.no_grad():\n",
        "    inputs = torch.tensor([word_to_ix[word] for word in training_data[0][0]], dtype=torch.long)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "\n",
        "# Define some test sentences and their corresponding tags\n",
        "test_data = [\n",
        "    (\"The dog read the book\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    for sentence, tags in test_data:\n",
        "        # Prepare the input sequence\n",
        "        inputs = torch.tensor([word_to_ix[word] for word in test_data[0][0]], dtype=torch.long)\n",
        "\n",
        "        # Pass the input sequence through the model\n",
        "        tag_scores = model(inputs)\n",
        "        print(tag_scores.argmax(dim=0).numpy())\n",
        "\n",
        "        # Get the predicted tags by taking the argmax of the tag scores\n",
        "\n",
        "        predicted_tags = [ix_to_tag[tag] for tag in tag_scores.argmax(dim=1).numpy()]\n",
        "\n",
        "        # Print the sentence, true tags, and predicted tags\n",
        "        print(\"Sentence: \", sentence)\n",
        "        print(\"True tags: \", tags)\n",
        "        print(\"Predicted tags: \", predicted_tags)\n",
        "        print()\n",
        "\n"
      ],
      "metadata": {
        "id": "8L6i1HSrVJix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb041ef1-65dd-4b8b-d687-66240234f7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'V': 0, 'DET': 1, 'NN': 2}\n",
            "{0: 'V', 1: 'DET', 2: 'NN'}\n",
            "tensor([[-2.9095, -0.1710, -2.2765],\n",
            "        [-4.1558, -4.0201, -0.0342],\n",
            "        [-0.0291, -4.4511, -4.0730],\n",
            "        [-4.1403, -0.0289, -4.3800],\n",
            "        [-4.6887, -3.8625, -0.0307]])\n",
            "[2 3 4]\n",
            "Sentence:  ['The', 'dog', 'read', 'the', 'book']\n",
            "True tags:  ['DET', 'NN', 'V', 'DET', 'NN']\n",
            "Predicted tags:  ['DET', 'NN', 'V', 'DET', 'NN']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IETG33QTkVkC",
        "outputId": "e2a1f3bf-f976-4063-fd8f-39b0b5f80579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'dog', 'ate', 'the', 'apple']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class LSTMTaggerWithAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTaggerWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
        "        self.attention = nn.Linear(hidden_dim*2, 1)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "\n",
        "        # Compute attention weights and weighted representation\n",
        "        attn_weights = F.softmax(self.attention(lstm_out), dim=0).transpose(0, 1)\n",
        "        attn_applied = torch.bmm(attn_weights, lstm_out.transpose(0, 1))\n",
        "\n",
        "        tag_space = self.hidden2tag(attn_applied.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "# Define training data and word-to-index and tag-to-index dictionaries\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {word: i for i, word in enumerate(set(word for sent, _ in training_data for word in sent))}\n",
        "tag_to_ix = {tag: i for i, tag in enumerate(set(tag for _, tags in training_data for tag in tags))}\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "print(tag_to_ix)\n",
        "print(ix_to_tag)\n",
        "\n",
        "# Define hyperparameters\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = LSTMTaggerWithAttention(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(300):\n",
        "    for sentence, tags in training_data:\n",
        "        # Clear out gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Prepare inputs for the network\n",
        "        sentence_in = torch.tensor([word_to_ix[word] for word in sentence], dtype=torch.long)\n",
        "        targets = torch.tensor([tag_to_ix[tag] for tag in tags], dtype=torch.long)\n",
        "\n",
        "        # Forward pass\n",
        "        tag_scores = model(sentence_in.unsqueeze(0))\n",
        "\n",
        "        # Compute loss, gradients, and update parameters\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test the model on a sample sentence\n",
        "with torch.no_grad():\n",
        "    inputs = torch.tensor([word_to_ix[word] for word in training_data[0][0]], dtype=torch.long)\n",
        "    tag_scores = model(inputs.unsqueeze(0))\n",
        "    print(tag_scores)\n",
        "\n",
        "# Define some test sentences and their corresponding tags\n",
        "test_data = [\n",
        "    (\"The dog read the book\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    for sentence, tags in test_data:\n",
        "        # Prepare the input sequence\n",
        "        inputs = torch.tensor([word_to_ix[word] for word in test_data[0][0]], dtype=torch.long)\n",
        "\n",
        "        # Pass the input sequence through the model\n",
        "        tag_scores = model(inputs)\n",
        "        print(tag_scores.argmax(dim=0).numpy())\n",
        "\n",
        "        # Get the predicted tags by taking the argmax of the tag scores\n",
        "\n",
        "        predicted_tags = [ix_to_tag[tag] for tag in tag_scores.argmax(dim=1).numpy()]\n",
        "\n",
        "        # Print the sentence, true tags, and predicted tags\n",
        "        print(\"Sentence: \", sentence)\n",
        "        print(\"True tags: \", tags)\n",
        "        print(\"Predicted tags: \", predicted_tags)\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "TLZ7zJjfbSna",
        "outputId": "43c640e8-68de-43e5-e5c5-909f5c900e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'V': 0, 'DET': 1, 'NN': 2}\n",
            "{0: 'V', 1: 'DET', 2: 'NN'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-2263060ec042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Compute loss, gradients, and update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-2263060ec042>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Compute attention weights and weighted representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    695\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                            ):\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    699\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    208\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    211\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m    212\u001b[0m                     self.input_size, input.size(-1)))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 6, got 30"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Life is short, eat dessert first'\n",
        "\n",
        "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
        "print(dc)\n",
        "\n",
        "import torch\n",
        "\n",
        "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
        "print(sentence_int)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embed = torch.nn.Embedding(6, 16)\n",
        "embedded_sentence = embed(sentence_int).detach()\n",
        "\n",
        "print(embedded_sentence)\n",
        "print(embedded_sentence.shape)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "d = embedded_sentence.shape[1]\n",
        "\n",
        "d_q, d_k, d_v = 24, 24, 28\n",
        "\n",
        "W_query = torch.rand(d_q, d)\n",
        "W_key = torch.rand(d_k, d)\n",
        "W_value = torch.rand(d_v, d)\n",
        "\n",
        "x_2 = embedded_sentence[1]\n",
        "query_2 = W_query.matmul(x_2)\n",
        "key_2 = W_key.matmul(x_2)\n",
        "value_2 = W_value.matmul(x_2)\n",
        "\n",
        "print(query_2.shape)\n",
        "print(key_2.shape)\n",
        "print(value_2.shape)\n",
        "\n",
        "keys = W_key.matmul(embedded_sentence.T).T\n",
        "values = W_value.matmul(embedded_sentence.T).T\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)\n",
        "\n",
        "\n",
        "omega_24 = query_2.dot(keys[4])\n",
        "print(omega_24)\n",
        "\n",
        "omega_2 = query_2.matmul(keys.T)\n",
        "print(omega_2)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
        "print(attention_weights_2)\n",
        "\n",
        "context_vector_2 = attention_weights_2.matmul(values)\n",
        "\n",
        "print(context_vector_2.shape)\n",
        "print(context_vector_2)\n",
        "\n",
        "\n",
        "h = 3\n",
        "multihead_W_query = torch.rand(h, d_q, d)\n",
        "multihead_W_key = torch.rand(h, d_k, d)\n",
        "multihead_W_value = torch.rand(h, d_v, d)\n",
        "\n",
        "\n",
        "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
        "print(multihead_query_2.shape)\n",
        "\n",
        "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
        "print(stacked_inputs.shape)\n",
        "\n",
        "\n",
        "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs)\n",
        "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
        "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
        "print(\"multihead_values.shape:\", multihead_values.shape)\n",
        "\n",
        "\n",
        "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
        "multihead_values = multihead_values.permute(0, 2, 1)\n",
        "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
        "print(\"multihead_values.shape:\", multihead_values.shape)\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "d = embedded_sentence.shape[1]\n",
        "print(\"embedded_sentence.shape:\", embedded_sentence.shape)\n",
        "\n",
        "d_q, d_k, d_v = 24, 24, 28\n",
        "\n",
        "W_query = torch.rand(d_q, d)\n",
        "W_key = torch.rand(d_k, d)\n",
        "W_value = torch.rand(d_v, d)\n",
        "\n",
        "x_2 = embedded_sentence[1]\n",
        "query_2 = W_query.matmul(x_2)\n",
        "print(\"query.shape\", query_2.shape)\n",
        "\n",
        "keys = W_key.matmul(embedded_sentence.T).T\n",
        "values = W_value.matmul(embedded_sentence.T).T\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)\n",
        "\n",
        "\n",
        "\n",
        "embedded_sentence_2 = torch.rand(8, 16) # 2nd input sequence\n",
        "\n",
        "keys = W_key.matmul(embedded_sentence_2.T).T\n",
        "values = W_value.matmul(embedded_sentence_2.T).T\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fO0YUaib_3F",
        "outputId": "d68f4924-dbd7-493e-c7f0-59060e6f0192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n",
            "tensor([0, 4, 5, 2, 1, 3])\n",
            "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
            "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
            "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
            "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
            "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
            "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
            "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
            "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
            "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
            "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
            "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
            "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n",
            "torch.Size([6, 16])\n",
            "torch.Size([24])\n",
            "torch.Size([24])\n",
            "torch.Size([28])\n",
            "keys.shape: torch.Size([6, 24])\n",
            "values.shape: torch.Size([6, 28])\n",
            "tensor(11.1466)\n",
            "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800])\n",
            "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458])\n",
            "torch.Size([28])\n",
            "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
            "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
            "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
            "        -0.9564, -0.5265,  0.0624,  1.7084])\n",
            "torch.Size([3, 24])\n",
            "torch.Size([3, 16, 6])\n",
            "multihead_keys.shape: torch.Size([3, 24, 6])\n",
            "multihead_values.shape: torch.Size([3, 28, 6])\n",
            "multihead_keys.shape: torch.Size([3, 6, 24])\n",
            "multihead_values.shape: torch.Size([3, 6, 28])\n",
            "embedded_sentence.shape: torch.Size([6, 16])\n",
            "query.shape torch.Size([24])\n",
            "keys.shape: torch.Size([6, 24])\n",
            "values.shape: torch.Size([6, 28])\n",
            "keys.shape: torch.Size([8, 24])\n",
            "values.shape: torch.Size([8, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input sentence\n",
        "input_sentence = 'Life is short, eat dessert first'\n",
        "\n",
        "# Create a dictionary that maps each word to a unique index\n",
        "word_to_index = {word: i for i, word in enumerate(sorted(input_sentence.replace(',', '').split()))}\n",
        "\n",
        "# Convert the input sentence to a tensor of indices using the word_to_index dictionary\n",
        "input_tensor = torch.tensor([word_to_index[word] for word in input_sentence.replace(',', '').split()])\n",
        "\n",
        "# Define the embedding layer\n",
        "embedding_layer = torch.nn.Embedding(len(word_to_index), 16)\n",
        "\n",
        "# Embed the input tensor to get the embedded sentence\n",
        "embedded_sentence = embedding_layer(input_tensor).detach()\n",
        "\n",
        "# Define the size of the embedding\n",
        "embedding_size = embedded_sentence.shape[1]\n",
        "\n",
        "# Define the sizes of the query, key, and value vectors\n",
        "query_size, key_size, value_size = 24, 24, 28\n",
        "\n",
        "# Define the query, key, and value weight matrices\n",
        "query_weights = torch.rand(query_size, embedding_size)\n",
        "key_weights = torch.rand(key_size, embedding_size)\n",
        "value_weights = torch.rand(value_size, embedding_size)\n",
        "\n",
        "# Get the query, key, and value vectors for the second word in the input sentence\n",
        "x_2 = embedded_sentence[1]\n",
        "query_2 = query_weights.matmul(x_2)\n",
        "key_2 = key_weights.matmul(x_2)\n",
        "value_2 = value_weights.matmul(x_2)\n",
        "\n",
        "# Compute the keys and values for all words in the input sentence\n",
        "keys = key_weights.matmul(embedded_sentence.transpose(0, 1)).transpose(0, 1)\n",
        "values = value_weights.matmul(embedded_sentence.transpose(0, 1)).transpose(0, 1)\n",
        "\n",
        "# Compute the attention weights for the second word in the input sentence\n",
        "omega_2 = query_2.matmul(keys.T)\n",
        "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
        "\n",
        "\n",
        "# Compute the context vector for the second word in the input sentence\n",
        "context_vector_2 = attention_weights_2.matmul(values)\n",
        "\n",
        "# Define the number of attention heads\n",
        "num_heads = 3\n",
        "\n",
        "# Define the query, key, and value weight matrices for the multi-head attention\n",
        "multihead_query_weights = torch.rand(num_heads, query_size, embedding_size)\n",
        "multihead_key_weights = torch.rand(num_heads, key_size, embedding_size)\n",
        "multihead_value_weights = torch.rand(num_heads, value_size, embedding_size)\n",
        "\n",
        "# Get the query vector for the second word in the input sentence for each attention head\n",
        "multihead_query_2 = multihead_query_weights.matmul(x_2)\n",
        "\n",
        "# Repeat the embedded sentence for each attention head\n",
        "repeated_embedded_sentence = embedded_sentence.unsqueeze(0).repeat(num_heads, 1, 1)\n",
        "\n",
        "# Compute the keys and values for all words in the input sentence for each attention head\n",
        "multihead_keys = multihead_key_weights.matmul(repeated_embedded_sentence.transpose(1, 2)).transpose(1, 2)\n",
        "multihead_values = multihead_value_weights.matmul(repeated_embedded_sentence.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "# Compute the attention weights and context vectors for each attention head\n",
        "attention_weights = F.softmax(multihead_query_2.matmul(multihead_keys.transpose(1, 2)) / key_size**0.5, dim=2)\n",
        "context_vectors = attention_weights.matmul(multihead_values)\n",
        "\n",
        "# Create a random tensor to represent a second input sequence\n",
        "embedded_sentence_2 = torch.rand(8, 16)\n",
        "\n",
        "# Compute the keys and values for all words in the\n",
        "\n",
        "keys = W_key.matmul(embedded_sentence_2.T).T\n",
        "values = W_value.matmul(embedded_sentence_2.T).T\n",
        "\n"
      ],
      "metadata": {
        "id": "VPgk1pjQq6BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Ev5391Svld0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}